# 💠 强化学习概述

---

## 1. 什么是强化学习 Reinforcement Learning

<img src="https://gitee.com/veal98/images/raw/master/img/20201020170513.png" style="zoom: 45%;" />

⭐ **强化学习讨论的问题是一个 智能体 (`agent` / `actor`) 怎么在一个 复杂不确定的环境 `environment` 里面 去 极大化 它能获得的 奖励 (`reward`)。**

> 💡 李宏毅老师的课程中将智能体称为 `actor`

示意图由两部分组成：agent 和 environment。在强化学习过程中，agent 跟 environment 一直在交互。Agent 在环境里面获取到 **state (状态)**，agent 会利用这个状态输出一个 **action (动作)**。然后这个动作会放到环境之中去，环境会通过这个 agent 采取的动作，输出下一个状态以及当前的这个动作得到的**奖励 (reward)**。<u>Agent 的目的就是为了尽可能多地从环境中获取奖励</u>。

<img src="https://gitee.com/veal98/images/raw/master/img/20201110144915.png" style="zoom:60%;" />

## 2. 强化学习和监督学习

先看一下强化学习和机器学习的关系，**强化学习是机器学习大家族中的一大类**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021091905.png" style="zoom: 67%;" />

我们可以**把强化学习跟监督学习做一个对比**。举个图片分类的例子，监督学习就是说我们有一大堆标定的数据，比如车、飞机、凳子这些标定的图片，**这些图片都要满足独立同分布，就是它们之间是没有关联的一个分布**。然后我们训练一个分类器，比如说右边这个神经网络。为了分辨出这个图片是车辆还是飞机，训练过程中，我们把真实的 label 给了这个网络。当这个网络做出一个错误的预测，比如现在输入了这个汽车的图片，它预测出来是飞机。我们就会直接告诉它，你这个预测是错误的，正确的 label 应该是车。然后我们把这个错误写成一个`损失函数(loss function)`，通过 Backpropagation 来训练这个网络。

<img src="https://gitee.com/veal98/images/raw/master/img/20201020171713.png" style="zoom: 45%;" />

所以在监督学习过程中，有两个假设，

- 输入的数据，标定的数据，它都是没有关联的，尽可能没有关联。因为如果有关联的话，这个网络是不好学习的。
- 我们告诉这个 learner 正确的标签是什么，这样它可以通过正确的标签来修正自己的这个预测。

**在强化学习里面，这两点其实都不满足**。举一个 Atari Breakout 游戏的例子，这是一个打砖块的游戏，控制木板，然后把这个球反弹到上面来消除这些砖块：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020201007.png" style="zoom: 40%;" />

- 在游戏过程中，大家可以发现这个 **agent 得到的观测不是独立同分布，上一帧下一帧其实有非常强的连续性**。
- 另外一点，在玩游戏的过程中，你**并没有立刻获得这个反馈**。比如你现在把这个木板往右移，那么只会使得这个球往上或者往左上去一点，你并不会得到立刻的反馈。所以强化学习这么困难的原因是没有得到很好的反馈，但是你依然希望这个 agent 在这个环境里面学习。

强化学习的训练数据就是这样一个玩游戏的过程。你从第一步开始，采取一个决策，比如说你把这个往右移，接到这个球了。第二步你又做出决策，得到的 training data 是一个玩游戏的序列。比如现在是在第三步，你把这个序列放进去，你希望这个网络可以输出一个决策，在当前的这个状态应该输出往右移或者往左移：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020201812.png" style="zoom:50%;" />

这里有个问题，就是**我们没有标签来说明你现在这个动作是正确还是错误，必须等到这个游戏结束才能知道现在这个动作到底是不是对最后的输赢有帮助**。这里就面临一个 `延迟奖励(Delayed Reward)`，所以就使得训练这个网络非常困难。

🚩 **总结下强化学习和监督学习的区别**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020202907.png" style="zoom:40%;" />

- 🔸 首先强化学习输入的序列的数据并不是像 supervised learning 里面这些样本都是独立同分布的。

- 🔸 另外一点是 learner 并没有被告诉你每一步正确的行为应该是什么。Learner 不得不自己去发现哪些行为可以使得它最后得到这个奖励，只能通过不停地尝试来发现最有利的 action。

- 🔸 还有一点是 agent 获得自己能力的过程中，其实是通过不断地 **试错(trial-and-error exploration)**。Exploration 和 exploitation 是强化学习里面非常核心的一个问题。

  - **`Exploration 探索 ` 是说你会去尝试一些【新的行为】，这些新的行为有可能会使你得到更高的奖励，也有可能使你一无所有。**

  - **`Exploitation 利用 ` 说的是你就是就采取你【已知】的可以获得最大奖励的行为，你就重复执行这个 action 就可以了，因为你已经知道可以获得一定的奖励。**

    > 💡 比如说我们选择吃饭的餐馆：Environment 可理解为你可以选择的餐馆，agent 代表你，Reward 就是吃到好吃的美食：
    >
    > - `Exploitation `是去你喜欢的店吃，可以吃到味道不错的；
    >
    > - 而 `Exploration `代表尝试一家新的餐馆，这就意味着可能 reward 并不一定高

  因此，我们需要在 exploration 和 exploitation 之间取得一个**权衡（trade-off）**，这也是在监督学习里面没有的情况。

- 🔸 在强化学习过程中，没有非常强的 supervisor，只有一个 `奖励信号(reward signal)`，就是环境会在很久以后告诉你之前你采取的行为到底是不是有效的。Agent 在这个强化学习里面学习的话就非常困难，因为你没有得到即时反馈。

  当你采取一个行为过后，如果是监督学习，你就立刻可以获得一个指引，就比如说你现在做出了一个错误的决定，那么正确的决定应该是谁。而在强化学习里面，环境可能会告诉你这个行为是错误的，但是它并没有告诉你正确的行为是什么。而且更困难的是，它可能是在一两分钟过后才告诉你之前的行为到底行不行。所以这也是强化学习和监督学习不同的地方。

### Exploration and Exploitation 

在强化学习里面，`Exploration` 和` Exploitation` 是两个很核心的问题。上面已经说过了，此处只是为了目录的完整性列出来 😊

#### K-armed Bandit K-臂赌博机

与一般监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：**最大化单步奖赏**，即仅考虑一步操作。需注意的是，即便在这样的简化情形下，强化学习仍与监督学习有显著不同，**因为机器需通过尝试来发现各个动作产生的结果，而没有训练数据告诉机器应当做哪个动作**。

想要最大化单步奖赏需考虑两个方面：一是需知道每个动作带来的奖赏，二是要执行奖赏最大的动作。若每个动作对应的奖赏是一个确定值，那么尝试遍所有的动作便能找出奖赏最大的动作。然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值。

🚩 实际上，单步强化学习任务对应了一个理论模型，即 ` K-臂赌博机(K-armed bandit)`。K-臂赌博机也被称为 `多臂赌博机(Multi-armed bandit) `。如下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020225718.png" style="zoom: 67%;" />

K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。

- 若仅为获知每个摇臂的期望奖赏，则可采用 `仅探索(exploration-only)法` ：将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂)，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。
- 若仅为执行奖赏最大的动作，则可采用 `仅利用(exploitation-only)法`：按下目前最优的(即到目前为止平均奖赏最大的)摇臂，若有多个摇臂同为最优，则从中随机选取一个。

显然，仅探索法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

⭐ **事实上，探索 (即估计摇臂的优劣) 和 利用 (即选择当前最优摇臂) 这两者是矛盾的，因为尝试次数(即总投币数)有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的`探索-利用窘境(Exploration-Exploitation dilemma)`。显然，想要累积奖赏最大，则必须在探索与利用之间达成较好的折中**。

## 3. 强化学习的特征

通过跟监督学习比较，我们可以总结出强化学习的一些特征：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020203134.png" style="zoom:40%;" />

- 首先它是有 `trial-and-error exploration`，它需要通过探索环境来获取对这个环境的理解。
- 第二点是强化学习 agent 会从环境里面获得 **延迟奖励** `Delayed reward`。
- 第三点是**在强化学习的训练过程中，时间非常重要**。因为你得到的数据都是有这个时间关联的，而不是独立同分布的。<u>在机器学习中，如果观测数据有非常强的关联，其实会使得这个训练非常不稳定。这也是为什么在监督学习中，我们希望 data 尽量是独立同分布了，这样就可以消除数据之间的相关性</u>。
- 第四点是 **agent 的行为会影响它随后得到的数据**，这一点是非常重要的。在我们训练 agent 的过程中，很多时候我们也是通过正在学习的这个 agent 去跟环境交互来得到数据。所以如果在训练过程中，这个 agent 的模型很快死掉了，那会使得我们采集到的数据是非常糟糕的，这样整个训练过程就失败了。所以在强化学习里面一个非常重要的问题就是<u>怎么让这个 agent 的行为一直稳定地提升</u>。

## 4. Why Reinforcement Learning

为什么我们关注这个强化学习，其中非常重要的一点就是**强化学习得到的这个模型可以取得超人类的结果**。监督学习获取的这些监督数据，其实是让人来标定的。比如说 ImageNet，这些图片都是人类标定的。那么我们就可以确定这个算法的 upper bound (上限)，**人类的这个标定结果决定了它永远不可能超越人类**。但是对于强化学习，它在环境里面自己探索，有非常大的潜力，它可以获得超越人的能力的这个表现，比如说 AlphaGo。

💬 这里给大家举一些在现实生活中强化学习的例子：

- 国际象棋是一个强化学习的过程，因为这个棋手就是在做出一个选择来跟对方对战。
- 在自然界中，羚羊其实也是在做一个强化学习，它刚刚出生的时候，可能都不知道怎么站立，然后它通过 `trial- and-error` 的一个尝试，三十分钟过后，它就可以跑到每小时 36 公里，很快地适应了这个环境。
- 你也可以把股票交易看成一个强化学习的问题，怎么去买卖来使你的收益极大化。

## 5. 强化学习问题的 5 个基本对象

我们从概率角度描述强化学习过程，它包含了如下 5 个基本对象：

- **状态 𝑠** 反映了环境的状态特征，在时间戳 𝑡 上的状态记为 $s_t$，它可以是原始的视觉图像、语音波形等信号，也可以是高层抽象过后的特征，如小车的速度、位置等数据，所有的(有限)状态构成了状态空间 𝑆

- **动作 𝑎** 是智能体采取的行为，在时间戳𝑡上的状态记为 $a_t$，可以是向左、向右等离散动作，也可以是力度、位置等连续动作，所有的(有限)动作构成了动作空间 𝐴

- **策略 𝜋(𝑎|𝑠)** 代表了智能体的决策模型，接受输入为状态 𝑠，并给出决策后执行动作的概率分布 𝑝(𝑎|𝑠)，满足 <img src="https://gitee.com/veal98/images/raw/master/img/20201110144256.png" style="zoom: 45%;" />

  这种具有一定随机性的动作概率输出称为`随机性策略(Stochastic Policy)`。特别地，当策略模型总是输出某个动作的概率为 1，其它为 0 时，这种策略模型称为`确定性策略(Deterministic Policy)`，即 𝑎 = 𝜋(𝑠)

- **奖励 𝑟(𝑠, 𝑎)** 表达环境在状态 𝑠 时接受动作 𝑎 后给出的反馈信号，一般是一个标量值，它在一定程度上反映了动作的好与坏，在时间戳 𝑡 上的获得的激励记为 $r_t$(部分资料上记为 $𝑟_{𝑡+1}$，这是因为激励往往具有一定滞后性)

- **状态转移概率 𝑝(𝑠′|𝑠, 𝑎)** 表达了环境模型状态的变化规律，即当前状态 𝑠 的环境在接受动作 𝑎 后，状态改变为 𝑠′ 的概率分布，满足 <img src="https://gitee.com/veal98/images/raw/master/img/20201110144819.png" style="zoom:50%;" />

## 6. Gym 平台

强化学习算法可以通过大量的虚拟游戏环境来测试，为了方便研究人员调试和评估算法模型，OpenAI 开发了 Gym 游戏交互平台，用户通过 Python 语言，只需要少量代码即可完成游戏的创建与交互

### ① 安装

管理员身份打开 Anaconda Promt，进入 tensorflow 环境：

<img src="https://gitee.com/veal98/images/raw/master/img/20201102203831.png" style="zoom: 80%;" />

在配置好的 tensorflow 环境下安装 gym：

> TensorFlow 环境安装参见这篇文档 👉 [TensorFlow 2 快速入门](https://veal98.gitee.io/cs-wiki/#/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow2/TensorFlow2%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8)

```shell
pip install gym
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201102113908.png" style="zoom:67%;" />

### ② Gym 环境交互

一般来说，在 Gym 环境中创建游戏并进行交互主要包含了 5 个步骤：
1. **创建游戏**。通过 `gym.make(name)` 即可创建指定名称 name 的游戏，并返回游戏对象 env。
2. **复位游戏状态**。一般游戏环境都具有初始状态，通过调用 `env.reset()` 即可复位游戏状 态，同时返回游戏的初始状态 observation。
3. **显示游戏画面**。通过调用 `env.render()` 即可显示每个时间戳的游戏画面，一般用做测试。在训练时渲染画面会引入一定的计算代价，因此训练时可不显示画面。
4. **与游戏环境交互**。通过 `env.step(action)` 即可执行 action 动作，并返回新的状态 observation、当前奖励 reward、游戏是否结束标志 done 以及额外的信息载体 info。通过循环此步骤即可持续与环境交互，直至游戏回合结束。
5. **销毁游戏**。调用 `env.close()`即可。

💬 举个例子：

```python
import gym # 导入 gym 游戏平台
env = gym.make("CartPole-v1") # 创建平衡杆游戏环境
observation = env.reset() # 复位游戏，回到初始状态
for _ in range(1000): # 循环交互 1000 次
    env.render() # 显示当前时间戳的游戏画面
    action = env.action_space.sample() # 随机生成一个动作
    # 与环境交互，返回新的状态，奖励，是否结束标志，其他信息
    observation, reward, done, info = env.step(action)
    if done:#游戏回合结束，复位状态
    	observation = env.reset()
env.close() # 销毁游戏环境
```

## 7. 策略网络

下面我们来探讨强化学习中最为关键的环节：**如何判断和决策？**

**我们把判断和决策叫作 策略(Policy)**：策略的输入是状态 𝑠，输出为某具体的动作 𝑎 或动作的分布 $𝜋_𝜃(𝑎|𝑠)$，**其中 𝜃 为策略函数 𝜋 的参数，可以利用神经网络来参数化 $𝜋_𝜃$ 函数**。

如下图所示，图中神经网络 $𝜋_𝜃$ 的输入为平衡杆系统的状态 𝑠，即长度为 4 的向量，输出为所有动作的概率 $𝜋_𝜃(𝑎|𝑠)$：向左的概率 𝑃(向左|𝑠) 和向右的概率 𝑃(向右|𝑠)，并满足所有动作概率之和为 1 的关系：

<img src="https://gitee.com/veal98/images/raw/master/img/20201110150843.png" style="zoom: 60%;" />

我们将策略网络实现为一个 2 层的全连接网络，第一层将长度为 4 的向量转换为长度为 128 的向量，第二层将 128 的向量转换为 2 的向量，即动作的概率分布。和普通的神经网络的创建过程一样，代码如下

```python
import tensorflow
from tensorflow import keras

class Policy(keras.Model):
    def __init__(self):
        super(Policy, self).__init__()
        self.data = [] # 存储轨迹
        # 输入为长度为 4 的向量，输出为左、右 2 个动作，指定 W 张量的初始化方案
        self.fc1 = layers.Dense(128, kernel_initialize = 'he_normal')
        self.fc2 = layers.Dense(2, kernel_initializer='he_normal')
        # 网络优化器
        self.optimizer = optimizers.Adam(lr=learning_rate)
 
def call(self, inputs, training=None):
    # 状态输入 s 的 shape 为向量：[4]
    x = tf.nn.relu(self.fc1(inputs)) # 第一层激活函数relu
    x = tf.nn.softmax(self.fc2(x), axis=1) # 第二层激活函数softmax，获取动作的概率分布
    return x
```

在交互时，我们将每个时间戳上的状态输入 $𝑠_𝑡$，动作分布输出 $𝑎_𝑡$，环境奖励 $r_t$ 和新状态 $r_{t+1}$ 作为一个 4 元组 item 记录下来，用于策略网络的训练。代码如下：

```python
def put_data(self, item):
    # 记录 r,log_P(a|s)
    self.data.append(item)
```

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [莫烦 Python — 强化学习](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL-methods/)
- 👍 [Github - Deep-Learning-with-TensorFlow-book](https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book)

