# 🎤 模仿学习/示教学习 Imitation Learning

---

## 1. 什么是模仿学习

传统强化学习算法是一种通过不断与所处环境进行自主交互并从中得到策略的学习方式．然而，大多数多步决策问题难以给出传统强化学习所需要的反馈信号 reward．这逐渐成为强化学习在更多复杂问题中实现应用的瓶颈

举例来说，如果是棋类游戏或者是电玩，你有非常明确的 reward。但是其实多数的任务，都是没有 reward 的。以 chat-bot 为例，**机器跟人聊天，聊得怎么样算是好，聊得怎么样算是不好，你无法给出明确的 reward**。

`Imitation learning` 讨论的问题就是：**假设我们连 reward 都没有，那要怎么办呢？** 

Imitation learning 又叫做 `learning from demonstration(示范学习)` ，`apprenticeship learning(学徒学习)`，`learning by watching(观察学习)`。在 Imitation learning 里面，你有一些专家 expert 的示范，agent 也可以跟环境互动，但它没有办法从环境里面得到任何的 reward，它只能看着专家的示范来学习什么是好，什么是不好。

虽然没有办法给出 reward，但是收集专家 expert 的示范 demonstration 是可以做到的。举例来说，

- 在自动驾驶汽车里面，虽然你没有办法给出自动驾驶汽车的 reward，但你可以收集很多人类开车的纪录。
- 在 chat-bot 里面，你可能没有办法定义什么叫做好的对话，什么叫做不好的对话。但是收集很多人的对话当作范例，这一件事情也是可行的。

所以 imitation learning 的使用性非常高。imitation learning 可以被划分为以下３种实现方式：

- 🔸 `行为克隆 Behavior Cloning`

- 🔸 基于 `逆向强化学习 Inverse Reinforcement Learning` 的模仿学习方法，或者又叫做 `Inverse Optimal Control`

  > 💡 部分研究工作也认为逆强化学习是示教/模仿学习方法的一种，这是由于该类方法在工作过程中通过不断的正向策略搜索进而优化算法所需要的反馈信号， 因此整个系统（逆强化学习）可以被认为是一类示教/模仿学习方法．

- 🔸 基于`博弈`的模仿学习方法

  > 💡 经典的示教学习过程可以看作是智能体和所处环境进行博弈的过程．其中系统依据其混合策略$P_t$ 在动作空间选取动作，环境依据相应混合策略 $Q_t$ 选取状态，同时系统将观测到自身在执行决策之后所得到的损失值．相关的经典工作包括通过已有**自适应博弈方法优化学徒学习的 MWAL 算法**，以及**生成式对抗性 (Generative Adversarial ) 模仿学习方法（`GAN` for Imitation Learning，`GAIL`）**，通过生成器（generator）生成策略，由判别器（discriminator）判断其是否是来自专家决策数据抑或是生成器生成的策略数据，通过训练２个学习器，寻找最优策略．

## 2. 行为克隆 Behavior Cloning

### ① 概念

**其实 `Behavior Cloning` 跟监督学习是一模一样的**。以自动驾驶汽车为例，你可以收集到人开自动驾驶汽车的所有资料，比如说可以通过行车记录器进行收集。看到这样的场景的时候，人会决定向前。机器就采取跟人一样的行为，也向前，就结束了。**这个就叫做 Behavior Cloning，Expert 做什么，机器就做一模一样的事。**

<img src="https://gitee.com/veal98/images/raw/master/img/20201029114613.png" style="zoom: 50%;" />

怎么让机器学会跟 expert 一模一样的行为呢？就把它当作一个 supervised learning 的问题，你去收集很多行车记录器，然后再收集人在那个情境下会采取什么样的行为。你知道说人在 state $s_1$ 会采取 action $a_1$，人在 state $s_2$ 会采取 action $a_2$。人在 state $s_3$ 会采取 action $a_3$。接下来，你就训练一个 network。这个 network 就是你的 actor，它输入 $s_i$ 的时候，你就希望它的输出是 $a_i$

### ② DataSet Aggregation

Behavior Cloning 虽然非常简单，但它的问题是如果你只收集 expert 的资料，看过的场景会是非常有限的。

所以光是做 Behavior Cloning 是不够的，**只观察 expert 的行为是不够的，我们还需要 `Dataset Aggregation`**。

以自动驾驶汽车为例的话，假设一开始，你的 actor 叫作 π1，你让 π1 去开这个车。但车上坐了一个 expert。这个 expert 会不断地告诉 π1，如果在这个情境里面，我会怎么样开，但是 π1 自己开自己的。比如说，如上图所示，一开始的时候，expert 可能说往前走。在拐弯的时候，expert 可能就会说往右转。但 π1 是不管 expert 的指令的，所以它会继续去撞墙。我们要做的记录就是 expert 在 π1 看到的这种场景的情况下会做出什么样的行为，然后用这个数据去训练新的 actor $\pi_2$ 。这个过程反复继续下去就叫做 `Dataset Aggregation`。

<img src="https://gitee.com/veal98/images/raw/master/img/20201029122503.png" style="zoom:50%;" />

🚨 agent 可能会复制专家**所有的动作**，包括一些无关的动作。但是机器只有有限的学习能力，可能会导致它复制错误的行为。有些行为必须被复制，但有些可以被忽略，但监督学习对所有的误差都平等处理。

在监督学习中，我们希望训练数据和测试数据有相同的分布，但是在行为克隆中，训练数据来自于专家的分布，而测试数据来自于 actor，因为专家的 π 和 actor 的是不一样的，生成的 state 也是不一样的，分布可能会不相同，因此引入 IRL 👇 

## 3. 逆向强化学习 IRL

### ① 概念

在前面介绍过的 RL 中：环境和 reward 是用来生成一个 actor 的，但是在 IRL 中，没有 reward function，也就是说**机器是可以跟环境互动的，但它得不到 reward**。它的 reward 必须要从 expert 那边推出来，有了环境和 expert 示范 demonstration 以后，去反推出 reward function。然后这个 reward function 才会被用来训练 actor。

<img src="https://gitee.com/veal98/images/raw/master/img/20201029142407.png" style="zoom:50%;" />

Inverse Reinforcement Learning 实际上是怎么做的呢？

Expert 去玩一玩游戏，得到这一些游戏的纪录，你的 actor 也去玩一玩游戏，得到这些游戏的纪录。接下来，你要定一个 reward function，**这个 reward function 的原则就是 expert 得到的分数要比 actor 得到的分数高**。有了新的 reward function 以后，就可以套用一般 Reinforcement Learning 的方法去学习一个 actor，这个 actor 会针对 reward function 去最大化它的 reward，采取一大堆的动作。

接下来，我们接着改变 reward function。这个 actor 就会很生气，它已经可以在这个 reward function 得到高分。**但是它得到高分以后，我们就改 reward function，仍然让 expert 可以得到比 actor 更高的分数**。这个就是 `Inverse Reinforcement learning`。有了新的 reward function 以后，根据这个新的 reward function，你就可以得到新的 actor，新的 actor 再去跟环境做一下互动，它跟环境做互动以后， 你又会重新定义你的 reward function，让 expert 得到的 reward 比 actor 大。

也就是说：**专家永远是最棒的（专家决策最优）** 😊

### ② 反馈信号函数

然而，由于<u>在函数空间中可能存在多个函数能够同时满足专家策略最优的假设</u>，例如每一步决策所带来的反馈始终为０的情况．因此，算法设计的模型应能够解决**反馈信号的模糊性（ambiguity）**．目前，我们可以通过３类反馈信号函数的形式实现反馈信号求解过程，它们分别是：

- 🔸 １）基于大间隔（max margin）的反馈信号函数；

- 🔸 ２）基于确定基函数组合的反馈信号函数；

  > 💡 在数学中，**基函数**是函数空间中特定基底的元素。函数空间中的每个连续函数可以表示为基函数的线性组合，就像向量空间中的每个向量可以表示为基向量的线性组合一样。

- 🔸 ３）基于参数化模型的反馈信号函数，例如神经网络．

#### 基于确定基函数组合的反馈信号函数

逆强化学习发展初期，大多工作均建立在环境反馈信号函数为确定基函数组合的情况下．该类方法通过状态特征构建基函数，从而**将求解反馈信号函数的任务转化为求解各个基函数权重的任务**．其能够较好地克服反馈信号搜索过程中存在的函数歧义性的问题．

为了建立合适的优化模型求解相关决策问题的反馈信号，该类方法从专家决策轨迹最优的假设出发，通过以下２种方法建立相关模型：

- 🔹 **最优策略相对于其他任何策略，在状态空间上具有最大动作价值总和**。假设 $a_1$ 表示最优决策动作，即对于在动作集中任意非 $a_1$  的动作都有 <img src="https://gitee.com/veal98/images/raw/master/img/20201106162056.png" style="zoom: 50%;" /> 的值最大。因此，逆强化学习的目标也就是使得专家决策数据所确定的策略具有最高Ｑ总和

- 🔹 **最优策略相对于其他策略而言将获得最大的未来奖赏**。即 <img src="https://gitee.com/veal98/images/raw/master/img/20201106162256.png" style="zoom:52%;" /> 将取得最大值。

#### 基于参数化模型的反馈信号函数

随着逆强化学习面对的决策问题复杂度的提升，研究人员开始关注于提升反馈信号函数的表达 能力．其中较为有效的是通过参数化模型对环境反馈信号进行建模。**随着深度学习的蓬勃发展，通过神经网络对反馈函数进行建模逐渐成为逆强化学习的一大主流方向**

其中较为知名的是 2008 年 Ziebart 等人提出的 `最大熵逆强化学习方法（maximum entroy IRL）`， 通过优化专家决策数据集的似然函数实现反馈信号的优化，很好地解决了专家决策数据中可能存在的 噪声以及专家数据本身并不是最优的问题． 

最大熵逆强化学习方法是经典的基于“能量”的模型（energy-based model）．其中能量函数 ε 为环境的代价函数（即反馈信号函数的相反数）．根据 “能量”模型的假设，可以知道专家在策略轨迹空间的采样概率密度为

<img src="https://gitee.com/veal98/images/raw/master/img/20201106162919.png" style="zoom:57%;" />

其中，τ 为策略轨迹，分母为划分函数Ｚ（partition function）．上式可以简单地理解为：**当２条决策轨迹具有相同的反馈奖赏时，其具有相同的概率被 “专家” 采样获得，而当某条轨迹具有更高的反馈奖赏时，“专家”将更有机会能够采样到这条轨迹**

> 💡 此处需要注意的是： 当我们面对的是离散且规模较小的状态空间时，划分函数Ｚ可以通过动态规划算法求得；而当我们面对大规模状态空间时，则需要通过采样等方法实现

为了让专家决策数据（训练数据）更能够被 “专家” 采样到，**逆强化学习的优化目标是最大化专家轨迹的似然函数**，可以表述为：

<img src="https://gitee.com/veal98/images/raw/master/img/20201106163119.png" style="zoom:57%;" />

#### 其他函数表示形式

对于某些复杂决策问题，环境反馈信号难以通过单一的一个函数进行表示，也就是说是通过单一函数拟合过程中会出现决策数据和反馈函数严重不一致的情况．关于这些函数的表示形式此处就不介绍了

## 4. 基于逆强化学习的模仿学习方法

**模仿学习的目标是通过专家决策轨迹去模仿专家的决策行为**。

**逆强化学习是通过学习专家决策轨迹从而获得环境反馈信号的一类方法**。

本节将介绍通过结合逆强化学习、正向强化学习策略搜索算法所设计的模仿学习方法，也即基于逆强化学习的模仿学习方法．

目前，基于逆强化学习的模仿学习主要的２个框架分别是：

- １）在经典的正向强化学习算法内循环中使用逆强化学习算法优化问题的反馈信号，基于反馈信号函数继续实现策略的优化，不断迭代实现模仿学习过程．其核心在于将逆强化学习方法置于正向策略搜索方法的内循环之中，经典的方法包括 **学徒学习**方法等．

- ２）基于不断优化得到的反馈信号 去实现正向强化学习过程，通过采样数据和专家数 据相结合实现逆强化学习过程，同时将正向强化学 习过程置于逆强化学习的内循环中，经典的方法有 **代价指导学习**等

### ① 学徒学习

学徒学习方法是通过在马尔可夫决策过程中，模仿专家行为，最终得到不差于专家行为策略的方法．其核心的思想是通过**匹配专家期望特征**实现模仿学习过程．

<img src="https://gitee.com/veal98/images/raw/master/img/20201106164910.png" style="zoom:50%;" />

以下算法描述了由 Abbeel 等人提出的通过结合策略迭代和逆强化学习算法所设计的学徒学习方式．

<img src="https://gitee.com/veal98/images/raw/master/img/20201106165447.png" style="zoom:50%;" />

### ② 代价指导学习

代价指导学习是通过结合正向强化学习中的策略优化方法（policy optimization) 和最大熵逆强化学习方法实现的模仿学习方法

如图２所示，系统通过初始化策略在机器人或设备上进行轨迹采样，并将采样得到的轨迹和专家决策数据进行合并，共同用于实现逆强化学习过程， 优化反馈信号函数．基于得到的反馈信号函数，在内循环中实现策略优化．不断迭代上述过程，最终实现模仿学习过程．

<img src="https://gitee.com/veal98/images/raw/master/img/20201106165708.png" style="zoom:50%;" />

其实现过程如算法２所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20201106165802.png" style="zoom:50%;" />

## 📚 References

- 《基于逆强化学习的示教学习方法综述》— 张凯峰 俞 扬（南京大学）
- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)