# 🍍 马尔科夫决策过程 MDP

---

## 1. 前言

<img src="https://gitee.com/veal98/images/raw/master/img/2.2.png" style="zoom:40%;" />

上图介绍了在强化学习里面 agent 跟 environment 之间的交互，agent 在得到环境的状态过后，它会采取行为，它会把这个采取的行为返还给环境。环境在得到 agent 的行为过后，它会进入下一个状态，把下一个状态传回 agent。

⭐ 在强化学习中，agent 跟环境就是这样进行交互的，**这个交互过程是可以通过马尔科夫决策过程来表示的，所以马尔科夫决策过程是强化学习里面的一个基本框架**。在马尔科夫决策过程中，它的环境是 `完全可观测的 fully observable` 。虽然很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成 MDP 的问题。

在介绍马尔科夫决策过程 MDP 之前，先给大家梳理一下马尔科夫过程 MP、马尔科夫奖励过程 MRP。这两个过程是马尔科夫决策过程的一个基础：👇

<img src="https://gitee.com/veal98/images/raw/master/img/20201021100854.png" style="zoom: 25%;" />

## 2. 马尔科夫过程 Markov Process, MP

### ① 马尔科夫性质 Markov Property

⭐ 如果某一个过程满足`马尔科夫性质(Markov Property)`，就是说未来的转移跟过去是独立的，即**当前状态就能够决定未来，不需要历史信息**。可以用下面的状态转移概率公式来描述马尔科夫性：（$S_t$ 表示 t 时刻的状态）

<img src="https://gitee.com/veal98/images/raw/master/img/20201021101427.png" style="zoom:72%;" />

> 💡 $P[S_{t+1}|S_t]$ 表示从 $S_t$ 状态转移到 $S_{t+1}$ 状态的概率/可能性

**马尔科夫性质是所有马尔科夫过程的基础。**

### ② 马尔科夫链 Markov Chain

💡 <u>马尔科夫过程 Markov Process 也称马尔科夫链 Markov Chain，它是一个无记忆的随机过程，可以用一个**二元组 `<S,P>`** 表示，其中 S 是有限数量的状态集，P 是状态转移概率矩阵。</u>

举个例子，这个图里面有四个状态，这四个状态从 $s_1,s_2,s_3,s_4$ 之间互相转移。

<img src="https://gitee.com/veal98/images/raw/master/img/20201021102754.png" style="zoom:50%;" />

比如说从 $s_1$ 开始：

*  $s_1$ 有 0.1 的概率继续存活在 $s_1$ 状态，
*  有 0.2 的概率转移到 $s_2$， 
*  有 0.7 的概率转移到 $s_4$ 。

如果 $s_4$ 是我们当前状态的话，

* 它有 0.3 的概率转移到 $s_2$ ，
* 有 0.2 的概率转移到 $s_3$ ，
* 有 0.5 的概率留在这里。

我们可以用 `状态转移矩阵(State Transition Matrix)` P 来描述这样的**状态转移**。状态转移矩阵类似于一个 conditional probability，当我们知道当前我们在 $s_t$ 这个状态过后，到达下面所有状态的一个概率。所以它**每一行其实描述了是从一个节点到达所有其它节点的概率**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021102805.png" style="zoom: 33%;" />

下图是一个马尔科夫链的例子，我们这里有七个状态。从 $s_1$ 开始到 $s_2$ ，它有 0.4 的概率，然后它有 0.6 的概率继续存活在它当前的状态。 $s_2$ 有 0.4 的概率到左边，有 0.4 的概率到 $s_3$ ，另外有 0.2 的概率存活在现在的状态：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021102849.png" style="zoom:40%;" />

给定了这个状态转移的马尔科夫链后，我们可以对这个链进行采样，这样就会得到一串的 **轨迹**，称为 `Sample Episodes`。下面我们有三个轨迹，都是从同一个起始点开始。假设还是从 $s_3$ 这个状态开始：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021103425.png" style="zoom:50%;" />

## 3. 马尔科夫奖励过程 Markov Reward Process, MRP

### ① 概念

**`马尔科夫奖励过程(Markov Reward Process, MRP)` 是马尔科夫链/马尔科夫过程 MP 再加上了一个奖励函数(reward function)。**奖励函数是一个期望，就是说当你到达某一个状态的时候，可以获得多大的奖励。马尔科夫奖励过程可使用**四元组 `<S, P, R, γ>`** 表示。

> 💡 这里另外定义了一个 **【折扣系数/衰减因子 discount factor $\gamma$】** ，下文我们会解释这个参数有何意义

<img src="https://gitee.com/veal98/images/raw/master/img/2.7.png" style="zoom:35%;" />

下图是我们刚才看的马尔科夫链，如果把奖励也放上去的话，就是说到达每一个状态，我们都会获得一个奖励。比如说到达 $s_1$ 状态的时候，可以获得 5 的奖励，到达 $s_7$ 的时候，有 10 的奖励，其它状态没有任何奖励。因为这里状态是有限的，所以我们可以用一个向量 R 来表示这个奖励函数，**这个向量表示了每个点的奖励的大小**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021104852.png" style="zoom:40%;" />

### ② 收益/回报 Return

**【收益/回报 Return 的定义】**：在一个马尔科夫奖励链上从 t 时刻开始往后所有的奖励的有衰减的总和。公式如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107101504.png" style="zoom: 67%;" />

其中 `horizon`：它说明了一个轨迹的长度，它是由有限个步数决定的。

注意计算 Return 的时候我们定义了一个 **【折扣系数/衰减因子 discount factor $\gamma$】**，就是**越往后得到的奖励折扣得越多**。<u>这说明我们其实更希望得到现有的奖励，未来的奖励就要把它打折扣</u>。

> 💡 **解释一下为什么需要 `discounted factor` $γ$**：
>
> * 有些马尔科夫过程是带环的，它并没有终结，我们想避免这个无穷的奖励。
>
> * 我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。
>
> * 如果这个奖励是有实际价值的，我们可能是更希望立刻就得到奖励（即时奖励），而不是后面再得到奖励。
>
>   💡 γ∈[0,1]，越往后 $\gamma^n$ 就会越小，**越后面的收益对当前价值的影响就会越小，也就是说我们更希望获得即使的收益**。举个股票的例子：
>
>   ![](https://gitee.com/veal98/images/raw/master/img/20201028100853.png)
>
> * 在有些时候可以把这个系数设为 0。设为 0 过后，我们就只关注了它当前的奖励。我们也可以把它设为 1，设为 1 的话就是对未来并没有折扣，未来获得的奖励跟当前获得的奖励是一样的。这个系数其实可以作为强化学习 agent 的一个 hyperparameter 来进行调整，然后就会得到不同行为的 agent。

### ③ 状态价值函数 State Value Function

价值函数给出了某一状态或某一行为的**长期价值**。

当我们有了这个 Return 过后，就可以正式定义**一个状态的价值**了，就是 `状态价值函数 state value function` ，简称 `价值函数 value  funtion`。对于这个 MRP，它里面 R 的定义是**关于这个 return 的期望**， 期望就是说从这个状态开始，你有可能获得多大的价值。

> 💡 约定用 **状态价值函数 state-value function** 或 **价值函数 value function** 来描述针对状态的价值；用**行为价值函数 action-value function** 来描述某一状态下执行某一行为的价值。

⭐ **状态价值函数 (State Value Function，简称 V 函数)，它定义为从状态 $𝑠_𝑡$ 开始，在策略 $𝜋$ 控制下能获得的期望回报值**。价值函数的定义公式如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107101514.png" style="zoom:67%;" />

💬 举个例子，在下图这个 MRP 里面，如何计算它的价值：

<img src="https://gitee.com/veal98/images/raw/master/img/2.11.png" style="zoom:45%;" />

### ④ Bellman Equation 贝尔曼等式 

先尝试用价值的定义公式来推导看看能得到什么：

![](https://gitee.com/veal98/images/raw/master/img/20201107102720.png)

这个推导过程相对简单，仅在导出最后一行时，将 $G_{t+1}$变成了 $v(S_{t+1})$。其理由是 Return 的期望等于 Return 的期望的期望。因此，针对 MRP 的 Bellman 等式为：

![](https://gitee.com/veal98/images/raw/master/img/20201107103214.png)

通过方程可以看出 v(s) 由两部分组成：

- 一是该状态的即时奖励期望，**即时奖励期望等于即时奖励，因为根据即时奖励的定义，它与下一个状态无关**；
- 另一个是下一时刻状态的价值期望，可以根据下一时刻状态的概率分布得到其期望。

如果<u>用 s’ 表示 s 状态的**下一时刻任一可能的状态**</u>，那么 Bellman 等式可以写成：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107103357.png" style="zoom:67%;" />

其中：

*  $P(s'|s)$  是指从当前状态 s 转移到未来状态 $s'$ 的概率。
*  $V(s')$ 代表的是未来某一个状态的价值。我们从当前这个位置开始，有一定的概率去到未来的所有状态，乘以【未来所有状态】的价值，然后再乘以一个 $\gamma$，这样就可以把未来的奖励打折扣。

> 💡 **Bellman Equation 定义了当前状态跟未来状态之间的关系。表示当前状态的值函数可以通过下个状态的值函数来计算**。Bellman Equation 因其提出者、动态规划创始人 Richard Bellman 而得名 ，也叫作“**动态规划方程**”。

💬 举个例子，假设折扣系数 γ = 1：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107103721.png" style="zoom:125%;" />

## 4. 马尔科夫决策过程 Markov Decision Process, MDP

### ① 概念

**相对于 MRP，`马尔科夫决策过程(Markov Decision Process)`多了一个 `行为 Action`**，其它的定义跟 MRP 都是类似的。**下一个状态不仅依赖于当前的状态，也依赖于在当前状态 agent 采取的行为。价值函数 R 也同样的依赖于当前状态和行为**。马尔科夫决策过程可以用一个**五元组 `<S, A, P, R, γ>`** 表示

<img src="https://gitee.com/veal98/images/raw/master/img/2.18.png" style="zoom:43%;" />

我们用状态转移概率 $p\left[s_{t+1}, r_{t} \mid s_{t}, a_{t}\right]$ 来表述说在 $s_t$ 的状态选择了 $a_t$ 的动作的时候，转移到 $s_{t+1}$，而且拿到奖励 $r_t$ 的概率是多少。

> 💡 MDP 的 P 和 R 都与具体的**行为** a 对应，而不像马尔科夫奖励过程那样仅对应于某个**状态**

🐻 举个形象的例子：在 t-1 时刻，我看到了熊对我招手（状态 $S_{t-1}$），那我下意识的可能输出的动作就是赶紧跑路（动作 $a_{t-1}$)。熊看到了有人跑了，开始发动攻击（状态 $S_t$)。而在 t 时刻的话，我如果选择装死的动作（动作 $a_{t}$)，熊可能会走开（状态 $S_{t+1}$)。这个时候，我再跑路的话可能就跑路成功了，MDP 就是这样的一个序列决策的过程。

<img src="https://gitee.com/veal98/images/raw/master/img/20201025100138.png" style="zoom: 67%;" />

**每一个动作都可能对应几种不同的状态**，不是说你跑路了就能百分百活下来。比如说在 t 时刻，熊已经追上来了，我选择跑路，这个动作对应两种不同的状态：一种情况是我有一定的概率可以逃跑成功，另一种情况是也有很大的概率我会逃跑失败。

<img src="https://gitee.com/veal98/images/raw/master/img/20201025100954.png" style="zoom: 67%;" />

### ② MDP 和 MP/MRP 的差异

💡 **对比一下 MDP 里面的状态转移跟 MRP 以及 MP 的差异：**

<img src="https://gitee.com/veal98/images/raw/master/img/20201021145450.png" style="zoom:35%;" />

- 马尔科夫过程 MP 的转移是直接就决定。比如当前状态是 s，那么就直接通过转移概率 P 决定了下一个状态是什么。
- **但对于 MDP，它的中间多了一层行为 a** ，就是说在你当前这个状态的时候，首先要决定的是采取某一种行为，因为对于采取哪种行为你有一定的不确定性，所以你到未来的状态其实也是一个概率分布。**也就是说在当前状态跟未来状态转移过程中这里多了一层决策性 `Policy`，这是 MDP 跟之前的马尔科夫过程 MP 很不同的一个地方。**

### ③ Policy in MDP

<img src="https://gitee.com/veal98/images/raw/master/img/20201021145351.png" style="zoom:43%;" />

解释一下上图中的符号：

- 💡 `policy π` 表示的是在给定的 state 下，关于 action 的概率分布。就是说，**π 表示在一个状态 s 下，agent 接下来可能会采取的任意一个 action 的概率分布**。

  例如，在状态 s 下，agent 接下来要么向右走，要么向左走(有且只有两种情况)，可能 agent有 0.9 的概率往右走，0.1的概率往左走。 `π(a|s)` 表示的就是这种概率。

  而如果只是 π 的话，就表示所有的状态的概率形成的整个策略。因为对于每一个状态 s 都会有这样一个π(a|s) ，所有状态的 π(a|s) 就形成整体策略 π。比如，我们可以说我们的策略就是在每个状态下都随机选取一个动作，这也是一种策略。我们也可以说我们的策略就是在每个状态下一直选某一个动作。总之，**策略 π 是指所有状态都要使用这个策略，不是单独指某一个状态**。

- 💡 $P^\pi(s'|s,a)$ 其实是一个联合概率，**表示在执行策略 π 的情况下，状态从 s 转移到 s’ 的概率。**

  例如我们种一棵树，在种的过程中有以下四个状态(状态空间S)：缺水(s1)，健康(s2)，溢水(s3)，死亡(s4)。我们的动作(动作空间 A)有：浇水(a1)，不浇水(a2)。

  现在假如这棵树处于缺水状态，所以 π(a1|s1) 表示在缺水状态 s1 下，我有多大的概率采取浇水 a1 这个动作。在这里要注意的是，**采取动作了并不表示状态就会改变**，不浇水的话，树的状态可能是继续缺水，也可能是死亡。**浇水的话，树的状态也可能会继续缺水，也可能会变健康。也就是状态的转移也有一定的概率**，也就是状态转移概率 $Ps′|s$，所以 <img src="https://gitee.com/veal98/images/raw/master/img/20201021152224.png" style="zoom:50%;" /> 表示采取浇水动作 a 的概率与浇水之后树能从缺水状态 s 变到健康状态 s’ 的概率的乘积。

- 💡 奖励函数 <img src="https://gitee.com/veal98/images/raw/master/img/20201107105519.png" style="zoom: 50%;" /> 理解为：当前状态 s 下执行某一指定策略 $\pi$ 得到的即时奖励是该策略下所有可能行为得到的奖励与该行为发生的概率的乘积的和。

> 📋 策略Policy 在 MDP 中的作用相当于 agent 在某一个状态时做出选择，进而形成各种不同的马尔科夫过程，而且基于策略产生的每一个马尔科夫过程是一个马尔科夫奖励过程，各过程之间的差别是不同的选择产生了不同的后续状态以及对应的不同的奖励。

### ④ 价值函数 Value Function

#### Ⅰ 状态价值函数 State Value Function

<img src="https://gitee.com/veal98/images/raw/master/img/20201021160532.png" style="zoom: 67%;" />

顺着 MDP 的定义，我们可以把 `状态价值函数 state value function ` （简称 价值函数）进行一个定义，它的定义是跟 MRP 是类似的。

上图这个值函数表示**从状态 s 出发，使用策略 π 所带来的期望回报值**。<u>这个策略是指在一个状态下，执行所有可能行动的概率，是一个概率分布</u>。每个不同的状态的概率分布可以不一样，因为在不同的状态下，执行不同动作的概率不一样。

> 🚨 **注意这里的策略 π 并不是只针对当前状态 s 的，它是一个整体的策略，对于每个状态都有这样的策略。所以这里 π 是针对当前状态 s 下采取动作 a 之后的下一个状态以及以后的状态**。

#### Ⅱ 状态动作价值函数 State-Action Value Function / Q 函数

这里我们另外引入了一个 `Q 函数 / 状态动作值函数 (state-action value function)`。

⭐ **状态-动作值函数 <img src="https://gitee.com/veal98/images/raw/master/img/20201021152758.png" style="zoom:50%;" />(State-Action Value Function，简称 Q 函数)，它定义为从状态 $𝑠_𝑡$ 并执行动作 $𝑎_𝑡$ 的双重设定下，在策略 $𝜋$ 控制下能获得的期望回报值**

<u>行为价值函数一般都是与某一特定的状态相对应的，更精细的描述是**状态行为对**价值函数</u>。行为价值函数的公式描述如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021160556.png" style="zoom:67%;" />

#### Ⅲ 状态价值函数和状态动作价值函数的关系

<img src="https://gitee.com/veal98/images/raw/master/img/20201021160711.png" style="zoom: 67%;" />

上图白色点表示状态，黑色点表示动作，比如在这个 s 状态下，agent 可能执行左边的动作(假设概率0.4)，也可能执行右边的动作(假设概率 0.6)，那么 $v_π(s)$想表达的就是一种期望 reward。而这个 0.4 和 0.6 其实就是由策略 π 决定的。

而 $q_π(s,a)$ 表示的是执行一个具体的动作后的 reward 值。所以显然，$v_π(s)$与 $q_π(s,a)$的关 系就是：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021155138.png" style="zoom:60%;" />

比如种树那个例子，假如现在的状态是缺水，那么我的动作有两个，要么浇水，要么不浇水。**在还没有执行动作(即浇水或者不浇水) 之前的值函数就是 v。如果我已经浇水了，那么就是 q。采取不浇水的动作后，就是另一个 q。**

<img src="https://gitee.com/veal98/images/raw/master/img/20201021155430.png" style="zoom:50%;" />

这个图承接了上一个图，因为执行了动作 a 后，状态就可能会发生改变，这里有可能变成左边的状态(白色圆圈)，有可能变到右边的状态(白色圆圈)。而改变状态就会产生一个 r 的即时奖励，然后到达 s’ 状态，这样又回到了$v_π(s′)$，显然这是一个递归的过程。

等式右边说的是执行动作 a 之后，有可能去到哪个状态。去到不同状态的奖赏就不一样，所以期望的奖赏就是它们对应的概率乘以下一个状态的 $v_π(s′)$，然后所有可能的情况加起来就是期望的奖赏了。

💡 所以上面两个图结合起来就是：

- **对于 v 来说**：

  <img src="https://gitee.com/veal98/images/raw/master/img/20201021155627.png" style="zoom:50%;" />

- **对于 Q 来说**：

  <img src="https://gitee.com/veal98/images/raw/master/img/20201021155659.png" style="zoom:50%;" />

💬 下图解释了红色空心圆圈状态的状态价值是如何计算的，遵循的策略随机策略，即所有可能的行为有相同的几率 50% 被选择执行：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107110602.png" style="zoom:125%;" />

### ⑤ 贝尔曼期望等式 Bellman Expectation Equation

通过对状态-价值函数进行分解/推导，我们就可以得到一个类似于之前 MRP 的 Bellman Equation，这里叫 `Bellman Expectation Equation`。对于 Q 函数，我们也可以做类似的分解，也可以得到对于 Q 函数的 Bellman Expectation Equation：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021153714.png" style="zoom:40%;" />

### ⑥ 最优价值函数 Optimal Value Function

#### Ⅰ 最优价值/状态动作值函数

强化学习是通过奖励或惩罚来学习怎样选择能产生最大积累奖励的行动的算法。**为了找到最好的行动，非常有效的方式是，找到那些奖励最大的状态**，即**在我们目前的环境（environment）中首先找到最有价值的状态 states**。例如，在赛车跑道上最有价值的是终点线（这里好像就是你冲刺要达到 deadline 的前一步，这个状态肯定最有价值），这也是奖励最多的状态，因此在跑道之上的状态也比在跑道之外的状态更有价值。（其实这里面就是递归的思想，当你找到了最有价值的状态，你只需要想办法得到这个状态就好了）

<img src="https://gitee.com/veal98/images/raw/master/img/20201021204028.png" style="zoom: 80%;" />

- 🔹 最优状态价值函数 $v^∗(s)$ 指的是在所有的策略产生的状态价值函数中最大的那个函数。

- 🔹 同样的，最优状态动作值函数 $q^∗(s,a)$ 指的是在所有的策略中产生的状态动作价值函数中最大的那个函数。因为我们的目标就是要找到一个使得 reward 最大化的路径，所以也就是相当于每一步都要找到最大的。

#### Ⅱ 最优策略 Optimal Policy

当对于任何状态 s，遵循策略π的价值不小于遵循策略 π' 下的价值，则策略π优于策略 π’：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107114057.png" style="zoom: 80%;" />

**定理** 对于任何MDP，下面几点成立：

- 存在一个最优策略，比任何其他策略更好或至少相等；
- 所有的最优策略有相同的最优价值函数；
- 所有的最优策略具有相同的行为价值函数。

#### Ⅲ 寻找最优策略

如何寻找最优策略呢？

如果在一个状态 s 下，agent 可以向左走或者向右走，现在知道往右走可以得到 17 单位的 reward，也就是知道了 $q(s,a=rightmove)$ ，然后我又知道往左走可以得到 80 单位的 reward，也就是 $q(s,a=leftmove)$ 。那么不用想了，肯定是向左走的策略好呀。

所以，下图的公式表示的就是，**在 q 已经得到最大化时，我们在状态 s 下，要选择哪一个策略(动作)才能去到 $q^∗$ 那里**。也就是说，通过**最大化最优行为价值函数 q 来找到最优策略**。所以表示成下图：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021204501.png" style="zoom:40%;" />

<u>对于任何MDP问题，总存在一个确定性的最优策略；同时如果我们知道最优行为价值函数，则表明我们找到了最优策略。</u>

#### Ⅳ 贝尔曼最优等式 Bellman Optimality Equation

> 💡 同样的，我们对最优价值函数和最优状态动作值函数进行推导，得出 `贝尔曼最优等式 Bellman Optimality Equation`

针对 $v_*$ ，一个状态的最优价值等于从该状态出发采取的所有行为产生的行为价值中最大的那个行为价值：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107113446.png" style="zoom:67%;" />

针对 $q_*$ ，在某个状态 s 下，采取某个行为的最优价值由 2 部分组成，一部分是离开状态 s 的即刻奖励，另一部分则是所有能到达的状态 s’ 的最优状态价值按出现概率求和：

<img src="https://gitee.com/veal98/images/raw/master/img/20201107113748.png" style="zoom:67%;" />

组合起来：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021211000.png" style="zoom:40%;" />

上式被称为 `Bellman Optimality Equation`。**这个 Bellman Optimality Equation 满足的时候，是说整个 MDP 已经到达最佳的状态。**

Bellman最优方程是非线性的，没有固定的解决方案，通过一些迭代方法来解决：价值迭代、策略迭代、Q学习、Sarsa等。后续会逐步讲解展开。

## 5. 价值函数估计

在介绍完 Q 函数与 V 函数的定义后，我们主要来解答以下两个问题：

- 值函数怎么计算(估计)
- 由值函数怎么推导出策略

### ① 蒙特卡罗方法

<img src="https://gitee.com/veal98/images/raw/master/img/20201110210337.png" style="zoom:50%;" />

蒙特卡罗方法简单容易实现，但是需要获得回合的完整轨迹，计算效率较低，而且部分环境并没有明确的终止状态。

### ② 时序差分方法

<img src="https://gitee.com/veal98/images/raw/master/img/20201110210719.png" style="zoom:50%;" />

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [知乎 -《强化学习》第二讲 马尔科夫决策过程](https://zhuanlan.zhihu.com/p/28084942)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [CSDN - 强化学习(二)：马尔科夫决策过程(Markov decision process) - Webbley](https://blog.csdn.net/liweibin1994/article/details/79079884)