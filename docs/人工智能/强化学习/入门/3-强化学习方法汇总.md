# 🥛 强化学习方法汇总

---

了解强化学习中常用到的几种方法,以及他们的区别, 对我们根据特定问题选择方法时很有帮助. 强化学习是一个大家族, 发展历史也不短, 具有很多种不同方法. 比如说比较知名的控制方法 Q learning, Policy Gradients, 还有基于对环境的理解的 model-based RL 等等. 接下来我们通过分类的方式来了解他们的区别.

## 1. Model-Free 和 Model-Based

<img src="https://gitee.com/veal98/images/raw/master/img/20201030160019.png" style="zoom: 67%;" />

**针对是否需要对真实环境建模（理不理解所处环境），强化学习可以分为有模型学习和免模型学习：**

- 第一种是 `model-based(有模型)` RL，它通过学习这个状态的转移来采取措施。

  通俗来说：<u>有模型学习是指根据环境中的经验，构建一个虚拟世界，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可</u>

  <img src="https://gitee.com/veal98/images/raw/master/img/20201020222759.png" style="zoom:50%;" />

  🙆 像 DQN、AlphaGo 系列等都采用免模型学习

  💬 举个例子：假设我们已知熊看到人装死就一定会走的话，我们就称这个状态转移概率就是 100%。或者我们已知在熊发怒的情况下，跑路成功的概率大概 10%，跑路失败的概率 90%。这些概率都是我们事先已知的，称为有模型学习

- 另外一种是 ` model-free(免模型)` RL，它没有去直接估计这个状态的转移，也没有得到环境的具体转移变量。它通过学习 value function 和 policy function 进行决策。这种 model-free 的模型里面没有一个环境转移的一个模型。

  通俗来说：<u>免模型学习没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新行为策略，这样反复迭代直到学习到最优策略。</u>

  🙆 Model-free 的方法有很多, 像 Q learning, Sarsa, Policy Gradients 都是从环境中得到反馈然后从中学习

  💬 仍以熊为例子，现实世界中人类第一次遇到熊之前，我们根本不知道能不能跑得过熊，所以刚刚那个 10%、90% 的概率也就是虚构出来的概率。熊到底在什么时候会往什么方向去转变，我们是不知道的。<u>我们是处在一个未知的环境里的，也就是这一系列决策的 P 函数（概率函数）和 R 函数（奖励函数）是未知的，我们只能不断通过试错/探索去获取这些概率</u> 👇

  <img src="https://gitee.com/veal98/images/raw/master/img/20201025103044.png" style="zoom:67%;" />

  

**免模型学习的泛化性要优于有模型学习，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。**

有模型的强化学习方法可以对环境建模，使得该类方法具有独特魅力，即“**想象能力**”。在免模型学习中，智能体只能一步一步地采取策略，等待真实环境的反馈；而有模型学习可以在虚拟世界中预测出所有将要发生的事，并采取对自己最有利的策略，这也就是 围棋场上 AlphaGo 能够超越人类的原因。

## 2. Policy-Based 和 Value-Based

<img src="https://gitee.com/veal98/images/raw/master/img/20201030160305.png" style="zoom: 67%;" />

**基于概率/策略（`Policy-Based`）**是强化学习中最直接的一种, 他能通过感官分析所处的环境, 直接**输出下一步要采取的各种动作的概率, 然后根据概率采取行动**, 所以每种动作都有可能被选中, 只是可能性不同. <u>即使某个动作的概率最高, 但是还是不一定会选到他</u>

**基于价值（`Value-Based`）**的方法输出则是所有动作的价值, 我们会**根据最高价值来选择动作**, 相比基于概率的方法, 基于价值的决策部分更为铁定, 就选价值最高的。

> 💡 上节我们讲过，在强化学习中，有两类值函数：状态值函数和状态-动作值函数，两者均表示在策略 𝜋 下的期望回报，轨迹起点定义不一样。

![](https://gitee.com/veal98/images/raw/master/img/20201108212343.png)

我们现在说的动作都是一个一个不连续的动作, 🚨 而**对于选取连续的动作, 基于价值的方法是无能为力的，但是我们能用一个概率分布在连续动作中选取特定动作, 这也是基于概率的方法的优点之一** 。

那么这两类使用的方法又有哪些呢?

比如在基于概率这边, 有 Policy Gradients, 在基于价值这边有 Q learning, Sarsa 等. 而且我们还能结合这两类方法的优势之处, 创造更牛逼的一种方法, 叫做 Actor-Critic, actor 会基于概率做出动作, 而 critic 会对做出的动作给出该动作的价值, 这样就在原有的 policy gradients 上加速了学习过程

## 3. 回合更新 MC 和单步更新 TD

<img src="https://gitee.com/veal98/images/raw/master/img/20201030161724.png" style="zoom:67%;" />

想象强化学习就是在玩游戏, 游戏回合有开始和结束

![](https://gitee.com/veal98/images/raw/master/img/20201030160536.png)

- **回合更新（蒙特卡洛 Monte-Carlo）**指的是游戏开始后, 我们要一直等到游戏结束, 然后再总结这一回合中的所有转折点, 再更新我们的行为准则
- **单步更新（时序差分 Temporal-Difference）**则是在游戏进行中每一步都在更新, 不用等待游戏的结束, 这样我们就能边玩边学习了

再来说说方法, Monte-carlo learning 和基础版的 policy gradients 等 都是回合更新制, Q-Learning, Sarsa, 升级版的 policy gradients 等都是单步更新制. 因为单步更新更有效率, 所以现在大多方法都是基于单步更新. 比如有的强化学习问题并不属于回合问题.

## 4. 在线学习 On-Policy 和离线学习 Off-Policy

<img src="https://gitee.com/veal98/images/raw/master/img/20201030162306.png" style="zoom:67%;" />

![](https://gitee.com/veal98/images/raw/master/img/20201030161957.png)

所谓在线学习, 就是指 agent 必须本人在场, 并且一定是本人边玩边学习。最典型的在线学习就是 Sarsa

而离线学习是 agent 可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则, **离线学习 同样是从过往的经验中学习, 但是这些过往的经历没必要是自己的经历, 任何人的经历都能被学习**。或者我也不必要边玩边学习, 我可以白天先存储下来玩耍时的记忆, 然后晚上通过离线学习来学习白天的记忆.那么每种学习的方法又有哪些呢？最典型的离线学习就是 Q learning, 后来人也根据离线学习的属性, 开发了更强大的算法, 比如让计算机学会玩电动的 Deep-Q-Network。

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)
- [莫烦 Python — 强化学习](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL-methods/)