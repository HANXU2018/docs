# 🛸 近端策略优化算法 PPO

---

## 1. On-policy / Off-policy

在讲 PPO 之前，我们先讲一下 on-policy 和 off-policy 这两种训练方法的区别。 在强化学习里面，我们要学习的就是一个 agent/actor。

- 如果要 learn 的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做 `on-policy(同策略)`。
- 如果要 learn 的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做 `off-policy(异策略)`。

比较拟人化的讲法是如果要学习的那个 agent，一边跟环境互动，一边做学习这个叫 on-policy。 如果它在旁边看别人玩，通过看别人玩来学习的话，这个叫做 off-policy。

上一节介绍的策略梯度 Policy gradient 就是 on-policy 的做法，因为在做 policy gradient 时，我们需要有一个 agent、一个 policy 和一个 actor。这个 actor 先去跟环境互动去搜集资料，搜集很多的轨迹 $\tau$，根据它搜集到的资料去更新 policy 的参数。所以 policy gradient 是一个 on-policy 的算法。

<u>上一节我们说过，策略梯度有个非常耗时的一点，就是每更新一次参数后，就需要重新对数据进行采样</u>。所以为了减少时间**我们可以将策略梯度算法从 on-policy 变成 off-policy**。 也就是说用另外一个 policy（参数 θ′）， 另外一个 actor 去跟环境做互动。用 θ′ 收集到的数据去训练 θ，这意味着说我们可以把 θ′ 收集到的数据使用多次。

具体怎么做呢，就需要介绍 `importance sampling` 的概念 👇

## 2. Importance Sampling

假设有一个函数 $f(x)$，你要计算从 p 分布中抽样选取数据 x，然后计算 f(x) 的期望值：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027110317.png" style="zoom:50%;" />

但是，当我们只能通过另外一个 actor 得到抽样数据的时候，也就是说不从 p 分布中抽样，从另一个分布 q 中抽样选择数据。这样，$f(x)$ 的期望值计算方式就发生改变：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027111159.png" style="zoom:40%;" />

解释一下：因为是从 q 分布做抽样，所以从 q 里抽样出来的每一笔数据，你需要乘上一个 `weight` 来修正 p，q 这两个 分布的差异，`weight` 就是$ \frac{p(x)}{q(x)}$。

<u>这样，通过 Importance Sampling 的技巧，你就可以从 p 分布做抽样换成从 q 分布做抽样</u>。

🚨 虽然理论上你可以把 p 换成任何的 q。但是在实现上， **p 和 q 不能差太多，否则方差会出现较大的差别。**

<img src="https://gitee.com/veal98/images/raw/master/img/20201027112753.png" style="zoom:60%;" />

先基于原始的分布 p 计算函数的方差，然后计算引入不同分布 q 之后得到的函数方差，可以发现两者得出的方差表达式后面一项相同，主要差别在于前面那一项，如果分布 p 和 q 之间差别太大，会导致第一项的值较大或较小，于是造成两者较大的差别。

⚠ **如果 p , q 两个分布的差别过大，在训练的过程中就需要进行更多次数的采样**

## 3. 将 Importance Sampling 应用于 Policy Gradient

现在要做的事情就是把 importance sampling 用在 Policy Gradient。把 on-policy training 改成 off-policy training。之前我们是拿 θ 这个 policy 去跟环境做互动，抽样出轨迹 τ，然后计算 $R(\tau) \nabla \log p_{\theta}(\tau)$。

现在我们不用 θ 去跟环境做互动，有另外一个 policy θ′ 或者说 另外一个 actor 去跟环境做互动，告诉 θ 跟环境做互动会发生什么事。然后，借此来训练 θ。

**也就是说，我们要训练的是 θ ，θ′ 只是负责做 demo，负责跟环境做互动**。所以抽样出来的数据跟 θ 本身是没有关系的。θ 可以更新参数很多次，一直到 θ 训练到一定的程度，θ′ 再重新去做抽样，这就是 on-policy 换成 off-policy 的妙用。

<img src="https://gitee.com/veal98/images/raw/master/img/20201027114150.png" style="zoom:40%;" />

实际上更新参数/梯度的时候，我们的式子是长这样子的：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027152014.png" style="zoom:60%;" />

$A^{\theta}\left(s_{t}, a_{t}\right)$ 即 `Advantage function` (收益 reward 减去基准 baseline) 。它要估测的是，在状态 $s_t$ 采取动作 $a_t$ 是好的还是不好的。接下来后面会乘上 $\nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)$，也就是说如果 $A^{\theta}\left(s_{t}, a_{t}\right)$ 是正的，就要增加概率， 如果是负的，就要减少概率。

那现在用了 importance sampling 的技术把 on-policy 变成 off-policy，就从 θ 变成 θ′。所以我们需要加入一个 `weight` 进行修正：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027152906.png" style="zoom:55%;" />

那接下来，我们可以拆解 $p_{\theta}\left(s_{t}, a_{t}\right)$ 和 $p_{\theta'}\left(s_{t}, a_{t}\right)$，即

<img src="https://gitee.com/veal98/images/raw/master/img/20201027152959.png" style="zoom:55%;" />

于是我们得到下式：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027153015.png" style="zoom:55%;" />

**在不同的参数情况下，某一个状态 state 出现的概率几乎没有差别，因此可以将这一项近似地消掉**，这样我们就得到了将 Importance Sampling 应用到 Policy Gradient 后的**梯度更新公式**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027153147.png" style="zoom:57%;" />

现在我们得到一个新的目标函数：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027153226.png" style="zoom:57%;" />

> 💡 $J^{\theta^{\prime}}(\theta)$ ：括号里面的 θ 代表我们要去最优化 optimize 的参数。θ′ 是说我们拿 θ′ 去跟环境互动。

## 4. Proximal Policy Optimization (PPO)

上文我们说到， importance sampling 中如果 $p_{\theta}\left(a_{t} | s_{t}\right)$ 跟 $p_{\theta'}\left(a_{t} | s_{t}\right)$ 差太多的话，importance sampling 的结果就会不好。怎么避免它差太多呢？这个就是 `Proximal Policy Optimization (PPO) ` 在做的事情。

我们在训练的时候，多加入一个` 约束 constrain` 即 θ 和 θ’ 的 **KL 散度**（KL divergence），用来表示两个分布之间的区别，区别越大该值越大，那么施加在目标函数上的惩罚 β 也就越大，因此要尽量使得两个分布之间的差距小，才能保证较大的目标函数。

<img src="https://gitee.com/veal98/images/raw/master/img/20201027155303.png" style="zoom:50%;" />

<img src="https://gitee.com/veal98/images/raw/master/img/20201027160228.png" style="zoom:28%;" />

PPO 有一个前身叫做 `信任区域策略优化算法 TRPO(Trust Region Policy Optimization)`，TRPO 的式子如下式所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027155432.png" style="zoom:50%;" />

它与 PPO 不一样的地方是 KL divergence 摆的位置不一样，PPO 是直接放到你要最优化的目标函数里，然后你就可以用梯度上升的方法去 maximize 这个式子。但 TRPO 是把 KL divergence 当作一个区间约束，它希望 θ 跟 θ′ 的 KL divergence 小于一个 δ。

但是，如果你是基于梯度做最优化的话，有区间约束是很难处理的，所以 **PPO 在实现上比 TRPO 容易的多**。

上面我们介绍的 PPO 也称 **PPO1** 算法 或者 **PPO-Penalty** 算法，还有一种变种算法 **PPO2** 也称 **PPO-Clip** 👇

## 5. PPO-Clip

PPO-Clip 要去最大化的目标函数如下式所示，它的式子里面**没有** KL divergence：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027160641.png" style="zoom: 52%;" />

`clip` 这个函数的意思是说：

- 在括号里面有 3 项，如果第一项小于第二项的话，那就输出第二项 1−ε
- 第一项如果大于第三项的话，那就输出 1+ε
- ε 是一个超参数 hyperparameter，可以设成 0.1 或 设 0.2

也就是说第二项即蓝色的虚线必须在 1-ϵ  和 1+ϵ 之间：

<img src="https://gitee.com/veal98/images/raw/master/img/20201027161002.png" style="zoom:50%;" />

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)