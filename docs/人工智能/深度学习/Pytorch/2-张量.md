# ⛅ Pytorch 最基本的数据结构 — 张量 tensor

---

## 1. 前言

深度学习的应用往往以某种形式获取数据（例如图像或文本），并以另一种形式生成数据（例如标签，数字或更多文本）。**这个过程的第一步是将输入转换为浮点数**，如下图所示的第一步（也可以是其他类型的数据）。因为网络使用浮点数来处理信息，所以我们需要对真实世界的数据进行编码，使其成为网络可理解的形式，然后再将输出解码回我们可以理解并用于某种用途的形式。

<img src="https://gitee.com/veal98/images/raw/master/img/20201018154050.png" style="zoom: 50%;" />

**从一种数据形式到另一种数据形式的转换通常是由深度神经网络分层次学习的，这意味着我们可以将层次之间转换得到的数据视为一系列中间表示（intermediate representation）**。以图像识别为例，浅层的表示可以是特征（例如边缘检测）或纹理（例如毛发），较深层次的表征可以捕获更复杂的结构（例如耳朵、鼻子或眼睛）。

**通常，这种中间表示形式是浮点数的集合，这些浮点数表征输入并捕获数据中的结构，从而有助于描述输入如何映射到神经网络的输出**。这种表征是特定于当前任务的，可以从相关示例中学习。这些浮点数的集合及其操作是现代 AI 的核心。

在开始将数据转换为浮点输入之前，我们必须对PyTorch如何处理和存储数据（输入、中间表示以及输出）有深刻的了解。本章就帮助读者准确地理解这些原理。

为此，PyTorch引入了一个基本的数据结构：**张量（tensor）**。**张量是指将向量（vector）和矩阵（matrix）推广到任意维度**，如下图所示。与张量相同概念的另一个名称是多维数组（multidimensional array）。张量的维数与用来索引张量中某个标量值的索引数一致。

<img src="https://gitee.com/veal98/images/raw/master/img/20201018154802.png" style="zoom: 33%;" />

PyTorch并不是唯一能处理多维数组的库。NumPy是迄今为止最受欢迎的多维数组处理库，以至于它可以被当做数据科学的通用语言。事实上，PyTorch可以与NumPy无缝衔接，从而使得PyTorch能够与Python中的其他科学库（如[SciPy](https://www.scipy.org/)、[Scikit-learn](https://scikit-learn.org/stable)和[Pandas](https://pandas.pydata.org/)）进行高度的整合。

与NumPy数组相比，PyTorch的张量具有一些更强大功能，例如能够在GPU进行快速运算、在多个设备或机器上进行分布式操作以及跟踪所创建的计算图。所有这些功能对于实现现代深度学习库都很重要。

## 2. 张量基础

张量是PyTorch中基本的数据结构。**张量是一个数组，即一种存储数字集合的数据结构，这些数字可通过索引单独访问，也可通过多个索引进行访问**。

我们来实际观察一下python的列表（list）索引，以便将其与张量索引进行比较。以下代码展示了Python中三个数字的列表。

```python
a = [1.0, 2.0, 1.0]
```

你可以使用相应的索引（从0开始的）来访问列表的第一个元素：

```python
a[0] # 1.0
```

输入：

```python
a[2] = 3.0 
```

```python
a # [1.0, 2.0, 3.0]
```

使用列表来存储并处理向量（例如2D线的坐标）在简单的Python程序中是很常见的。但是，由于以下几个原因，这种做法可能不是最佳的：

- **Python中的数是完整（ full-fledged）的对象。** 浮点数只需要32位就可以在计算机上表示，而Python将它们封装（boxes）在具有引用计数等功能的完整Python对象中。如果只需要存储少量数字，这种做法就没问题，但是要想分配数百万个这样的数字效率就太低了。
- **Python中的列表用于对象的有序集合。** 没有定义高效计算两个向量点积或向量求和的操作。此外，Python列表无法优化其在内存中的布局，因为它们是指向Python对象（任何类型，而不仅仅是数字）的可索引指针集合。最后，Python列表是一维的，尽管我们可以创建列表的列表，但这种做法仍然效率很低。
- **与经过优化和编译的代码相比，Python解释器速度较慢。**使用可编译的低层语言（例如C）编写的优化代码可以更快地对大量数据进行数学运算。

由于这些原因，数据科学库依赖于NumPy或引入专用数据结构（例如PyTorch张量），这些结构提供了高效的数值数据结构的底层实现以及相关运算，并被封装成高级API。

从图像到时间序列、音频甚至文本，张量可以表示许多类型的数据。通过定义在张量上的操作（本章将探讨其中的一些操作），即使使用不是特别快高级语言（例如Python），也可以高效地进行切片（slice）和操作数据。

现在，你可以构建第一个PyTorch张量来观察其特性。这个张量包含三个1，没有特别的意义：

```python
import torch

a = torch.ones(3)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018155822.png" style="zoom: 67%;" />

<img src="https://gitee.com/veal98/images/raw/master/img/20201018155955.png" style="zoom:67%;" />

尽管从表面上看，此示例与 Python 列表并没有太大区别，但实际上情况完全不同。**Python 列表或数字元组（tuple）是在内存中单独分配的 Python 对象的集合**，如下图左侧所示。然而，**PyTorch 张量或 NumPy 数组是连续内存块上的视图（view），这些内存块存有未封装（unboxed）的 C 数值类型**，在本例中，如下图右侧所示，就是 32 位的浮点数（4字节），而不是 Python 对象。因此，包含100万个浮点数的一维张量需要 400 万个连续字节存储空间，再加上存放元数据（尺寸、数据类型等）的少量开销。

<img src="https://gitee.com/veal98/images/raw/master/img/20201018160216.png" style="zoom: 50%;" />

假设你要管理一个2D坐标列表来表示一个几何对象，例如三角形。你可以用一个一维张量将横坐标`x`存储在偶数索引中然后将纵坐标`y`存储在奇数索引中，而不是用 Python 列表存放坐标数字，如下所示：

```python
# 使用.zeros是获取适当大小的数组的一种方法
points = torch.zeros(6)
# 用所需的值覆盖这些0
points[0] = 1.0
points[1] = 4.0
points[2] = 2.0
points[3] = 1.0
points[4] = 3.0
points[5] = 5.0
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018160429.png" style="zoom: 67%;" />

还可以传入Python列表以达到相同的效果：

```python
points = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0])
```

获取第一个点的坐标：

```python
float(points[0]), float(points[1])
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018160557.png" style="zoom:67%;" />

以上做法是可以的，不过实际上往往让第一个索引直接索引一个二维点而不是点坐标。为此，可以使用**二维张量**：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018160649.png" style="zoom:67%;" />

上例将**列表的列表**传递给了张量构造函数。你可以通过 `shape` 获取一个张量的形状：

```python
points.shape
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018160729.png" style="zoom:67%;" />

它会告诉你沿每个维度的张量的大小。你还可以使用`zeros`或`ones`来初始化张量，同时用元组指定大小：

```python
points = torch.zeros(3, 2)
points
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018160807.png" style="zoom:67%;" />

现在，你可以使用两个索引访问张量中的单个元素：

```python
points = torch.FloatTensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
points[0,1]
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018161012.png" style="zoom:67%;" />

此代码返回数据集中第0个点的 y 坐标。你还可以像以前一样访问张量中的第一个元素，以获取第一个点的二维坐标：

```python
points[0]
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018161054.png" style="zoom:67%;" />

请注意，**输出结果是另一个张量，它是大小为 2 的一维张量，包含了`points`的第一行中的值**。上述输出是否将值复制到了新分配的内存块并将新的内存封装在新的张量对象中？答案是不，因为这样效率不高，尤其是如果有数百万个点数据。与之相反，**上述输出是相同数据块的仅限于第一行的视图（view）**。

## 3. 张量与存储 Storage

在本节中，我们将开始了解关于内部实现的信息。数值分配在连续的内存块中，由`torch.Storage`实例管理。存储（`Storage`）是一个一维的数值数据数组，例如一块包含了指定类型（可能是 float 或 int32）数字的连续内存块。PyTorch的张量（`Tensor`）就是这种存储（`Storage`）的视图（view），我们可以使用偏移量和每一维的跨度索引到该存储中。

**多个张量可以索引同一存储，即使它们的索引方式可能不同**，如下图所示。 实际上，当你在上节最后一个代码片段中获取`points[0]`时，你得到的是另一个张量，该张量与`points`索引相同的存储，只是不是索引该存储的全部并且具有不同的维数（一维与二维）。由于基础内存仅分配一次，所以无论`Storage`实例管理的数据大小如何，都可以快速地在该数据上创建不同的张量视图。

<img src="https://gitee.com/veal98/images/raw/master/img/20201018161940.png" style="zoom:50%;" />

接下来，你将看到在二维坐标点的例子中索引到存储是如何工作的。我们可以使用`storage`属性访问给定张量的存储：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
points.storage()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018162031.png" style="zoom: 67%;" />

**即使张量具有三行两列，但内部的存储却是大小为 6 的连续数组**。从这个意义上讲，张量知道如何将一对索引转换为存储中的某个位置。

还可以手动索引到存储中：

```python
points_storage = points.storage()

points_storage[0] # 1.0

points.storage()[1] # 4.0
```

🚨 **无法使用两个索引来索引二维张量的存储，因为存储始终是一维的，与引用它的任何张量的维数无关**。

因此，**更改存储的值当然也会更改引用它的张量的内容**：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
points.storage()[0] = 2.0
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018162406.png" style="zoom:67%;" />

我们几乎很少会直接使用存储实例，但是了解张量与存储之间的关系对于以后了解某些操作的代价很有帮助。当你要编写高效的 PyTorch 代码时，请牢记这一思维模型。

## 4. 尺寸 size、存储偏移 offset 与步长 stride

### ① 概念

除了存放存储外，为了索引存储，张量依赖于几条明确定义它们的信息：**尺寸（size）、存储偏移（storage offset）和步长（stride）**

- 尺寸（或按照NumPy中的说法：形状shape）是一个元组，表示张量每个维度上有多少个元素。
- **存储偏移是存储中与张量中的第一个元素相对应的索引。**
- **步长是在存储中为了沿每个维度获取下一个元素而需要跳过的元素数量。**

<img src="https://gitee.com/veal98/images/raw/master/img/20201018162722.png" style="zoom:50%;" />

您可以通过提供相应的索引来获得张量中的第二个点：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
second_point = points[1] # tensor([2., 1.])
second_point.storage_offset() # 2
```

结果张量在存储中的偏移为2（因为我们需要跳过第一个点，该点有两个元素），并且尺寸是包含一个元素的`Size`类的实例，因为结果张量是一维的。

<img src="https://gitee.com/veal98/images/raw/master/img/20201018163237.png" style="zoom:67%;" />

需要注意的是，张量尺寸信息与张量对象的`shape`属性中包含的信息相同：

<img src="https://gitee.com/veal98/images/raw/master/img/20201018163331.png" style="zoom:67%;" />

最后，步长是一个元组，表示当索引在每个维度上增加 1 时必须跳过的存储中元素的数量。例如，上例`points`张量的步长：

```python
points.stride() # (2, 1)

second_point.stride() # (1,)
```

**用下标`i`和`j`访问二维张量等价于访问存储中的`storage_offset + stride[0] * i + stride[1] * j`元素**。偏移通常为零，但如果此张量是一个可容纳更大张量的存储的视图，则偏移可能为正值。

### ② 克隆 clone

**更改子张量同时也会对原始张量产生影响**：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
second_point = points[1]
second_point[0] = 10.0
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018163836.png" style="zoom:67%;" />

**这种影响可能不总是我们想要的，所以我们可以克隆 `clone` 子张量得到新的张量（以避免这种影响）**：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
second_point = points[1].clone()
second_point[0] = 10.0
points
```

输出:

```
tensor([[1., 4.],
        [2., 1.],
        [3., 5.]])
```

### ③ 转置 t

我们现在试试转置操作 `t`。`points`张量沿每一行就是单个点坐标，沿每一列分别是（所有点的）x和y坐标，现在我们将其旋转以使沿每一列是单个点坐标：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
points_t = points.t()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018165014.png" style="zoom:67%;" />

你可以轻松地验证**两个张量共享同一存储**：

```python
id(points.storage()) == id(points_t.storage()) # True
```

并且它们**仅仅是尺寸和步长不同**：

```python
points.stride() # (2, 1)
points_t.stride() # (1, 2)
```

上述结果告诉我们，在`points`中将第一个索引增加1（即，从`points[0,0]`到`points[1,0]`）会沿着存储跳过两个元素，将第二个索引从`points[0,0]`到点`points[0,1]`会沿存储跳过一个元素。换句话说，存储将`points`张量中的元素逐行保存着。

⭐ **整个转置过程没有分配新的内存，仅通过创建一个步长顺序与原始张量不同的新的张量实例来实现转置**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201018165144.png" style="zoom: 50%;" />

### ④ 连续张量 contiguous

在PyTorch中进行转置不仅限于矩阵（即二维数组）。以翻转三维数组的步长和尺寸为例，**你可以通过 `transpose` 指定应需要转置的两个维度来转置多维数组**：

```python
some_tensor = torch.ones(3, 4, 5) # 3 维数组 4x5
some_tensor.shape, some_tensor.stride() # (torch.Size([3, 4, 5]), (20, 5, 1))
```

```python
some_tensor_t = some_tensor.transpose(0, 2)
some_tensor_t.shape, some_tensor_t.stride() # (torch.Size([5, 4, 3]), (1, 5, 20))
```

⭐ **从最右边的维开始，将其值存放在存储中的张量定义为连续 Contiguous 张量（例如沿着行存放在存储中的二维张量）**。连续张量很方便，因为你可以高效且有序地访问它们的元素而不是在存储中四处跳跃访问。（由于现代 CPU 中内存访问的工作原理，改善数据局部性可提高性能。即**连续张量满足局部性原理**）

> 💡 在`torch`模块下可进行张量上和张量之间的绝大多数操作，这些操作也可以作为张量对象的方法进行调用。例如，你可以通过`torch`模块使用 `transpose` 函数：
>
> ```python
> a = torch.ones(3, 2)
> a_t = torch.transpose(a, 0, 1)
> ```
>
> 或者调用`a`张量的方法：
>
> ```python
> a = torch.ones(3, 2)
> a_t = a.transpose(0, 1)
> ```
>
> 以上两种形式之间没有区别，可以互换使用。

在前例中，`points`是连续的，但其转置不是：

```python
points.is_contiguous(), points_t.is_contiguous() # (True, False)
```

**你可以使用`contiguous`方法从非连续张量获得新的连续张量**。 张量的内容保持不变，但步长和存储发生变化：

```python
points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])
points_t = points.t()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018170248.png" style="zoom:67%;" />

```python
points_t.storage()
```

转置后的存储并未发生变化:

<img src="https://gitee.com/veal98/images/raw/master/img/20201018170332.png" style="zoom: 67%;" />

```python
points_t.stride() # (1, 2)
```

从非连续张量获得新的连续张量: 👇 

```python
points_t_cont = points_t.contiguous() # 张量的内容不变
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018170532.png" style="zoom:67%;" />

```python
points_t_cont.stride() # (3, 1) 步长发生变化
```

```python
points_t_cont.storage() # 存储发生变化
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018170622.png" style="zoom:67%;" />

请注意，**新的存储对元素进行了重组以便逐行存放张量元素。步长也已改变以反映新的布局**。

## 5. 数据类型

张量构造函数（即`tensor`、`ones`、`zeros`之类的函数）的`dtype`参数指定了张量中的数据类型。数据类型指定张量可以容纳的可能值（整数还是浮点数）以及每个值的字节数。`dtype`参数被故意设计成类似于同名的标准NumPy 参数。以下是 `dtype` 参数的可能取值的列表：

- `torch.float32`或`torch.float` —— 32位浮点数
- `torch.float64`或`torch.double` —— 64位双精度浮点数
- `torch.float16`或`torch.half` —— 16位半精度浮点数
- `torch.int8` —— 带符号8位整数
- `torch.uint8` —— 无符号8位整数
- `torch.int16`或`torch.short` —— 带符号16位整数
- `torch.int32`或`torch.int` —— 带符号32位整数
- `torch.int64`或`torch.long` —— 带符号64位整数

每个`torch.float`、`torch.double`等等都有一个与之对应的具体类：`torch.FloatTensor`、`torch.DoubleTensor`等等。`torch.int8`对应的类是`torch.CharTensor`，而`torch.uint8`对应的类是`torch.ByteTensor`。`torch.Tensor`是`torch.FloatTensor`的别名，即默认数据类型为32位浮点型。

想要分配正确数字类型的张量，你可以指定合适的`dtype`作为张量构造函数的参数，如下所示：

```python
double_points = torch.ones(10, 2, dtype=torch.double)
short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018171543.png" style="zoom:67%;" />

你可以通过访问`dtype`属性来获得张量的数据类型：

```python
short_points.dtype # torch.int16
```

您还可以使用相应的转换方法将张量创建函数的输出转换为正确的类型，例如

```python
double_points = torch.zeros(10, 2).double()
short_points = torch.ones(10, 2).short()
```

或者用更方便的`to`方法：

```python
double_points = torch.zeros(10, 2).to(torch.double)
short_points = torch.ones(10, 2).to(dtype=torch.short)
```

在实现内部，`type`和`to`执行相同的操作，即“检查类型如果需要就转换（check-and-convert-if-needed）”，但是`to`方法可以使用其他参数。

你始终可以使用`type`方法将一种类型的张量转换为另一种类型的张量：

```python
points = torch.randn(10, 2) # 返回一个元素是0到1之间随机数的张量 10x2
short_points = points.type(torch.short)
```

## 6. 索引张量

我们已经知道`points[0]`返回一个张量，该张量包含`points`第一行所表示二维点。如果你需要获取一个包含除第一个点之外的所有点的张量怎么办？

如果是应用在标准Python列表上，那么上述任务就可以很简单地用区间索引来实现：

```python
some_list = list(range(6))
some_list[:]     # 所有元素
some_list[1:4]   # 第1（含）到第4（不含）个元素
some_list[1:]    # 第1（含）个之后所有元素
some_list[:4]    # 第4（不含）个之前所有元素
some_list[:-1]   # 最末尾（不含）元素之前所有元素
some_list[1:4:2] # 范围1（含）到4（不含），步长为2的元素Copy to clipboardErrorCopied
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018200302.png" style="zoom: 67%;" />

为了实现这个目标，你可以对PyTorch张量使用相同的表示，并且具有与NumPy和其他Python科学库一样的额外好处，即我们可以**对张量的每个维使用区间索引**：

```python
points[1:]    # 第1行及之后所有行，（默认）所有列
points[1:, :] # 第1行及之后所有行，所有列
points[1:, 0] # 第1行及之后所有行，仅第0列
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018200543.png" style="zoom:67%;" />

## 7. 与 Numpy 的互通性

与NumPy数组的这种**零拷贝互通性**是由于（PyTorch的）存储是遵守[Python缓冲协议](https://docs.python.org/3/c-api/buffer.html)的。

🔸 要从张量创建NumPy数组，请调用 `numpy()`

```python
points = torch.ones(3, 4)
points_np = points.numpy()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018200934.png" style="zoom:67%;" />

它返回尺寸、形状和数值类型正确的NumPy多维数组。有趣的是，**返回的数组与张量存储共享一个基础缓冲区**。因此，只要数据位于CPU RAM中，`numpy`方法就可以几乎零花费地高效执行，并且修改得到的NumPy数组会导致原始张量发生变化。

如果在GPU上分配了张量，（调用`numpy`方法时）PyTorch会将张量的内容复制到在CPU上分配的NumPy数组中。

🔸 相反，你可以通过 `from_numpy` 从NumPy数组创建PyTorch张量，`from_numpy`使用相同的缓冲共享策略：

```python
points = torch.from_numpy(points_np)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018201009.png" style="zoom:67%;" />

## 8. 序列化张量

态创建张量是很不错的，但是如果其中的数据对你来说具有价值，那么你可能希望将其保存到文件中并在某个时候加载回去。毕竟你可不想每次开始运行程序时都从头开始重新训练模型！**PyTorch内部使用`pickle`来序列化张量对象和实现用于存储的专用序列化代码**。下面展示怎样将`points`张量保存到 ourpoints.t 文件中:

```python
torch.save(points, '../../data/chapter2/ourpoints.t')
```

或者，你也可以传递文件描述符代替文件名：

```python
with open('../../data/chapter2/ourpoints.t','wb') as f:
    torch.save(points, f)
```

将`points`加载回来也是一行类似代码：

```python
points = torch.load('../../data/chapter2/ourpoints.t')
```

等价于

```python
with open('../../data/chapter2/ourpoints.t','rb') as f:
    points = torch.load(f)
```

如果只想通过PyTorch加载张量，则上述例子可让你快速保存张量，**但这个文件格式本身是不互通（interoperable）的，你无法使用除PyTorch外其他软件读取它**。根据实际使用情况，上述情况可能问题不大，但应该学习一下如何在有的时候（即想用其他软件读取的时候）互通地保存张量。尽管实际情况都是独一无二的，但当你想将PyTorch引入已经依赖于不同库的现有系统中时，上述情况会很常见；而全新的项目可能不需要经常互通地保存张量。

对于需要（互通）的情况，你可以使用[HDF5](https://www.hdfgroup.org/solutions/hdf5)格式和库。HDF5是一种可移植的、广泛支持的格式，用于表示以嵌套键值字典形式组织的序列化多维数组。Python通过[h5py库](http://www.h5py.org/)支持HDF5，该库以NumPy数组的形式接收和返回数据。

你可以使用以下命令安装`h5py`：

```shell
conda install h5py
```

此时，你可以通过将`points`张量转换为NumPy数组（如前所述，此操作几乎零花费）并将其传递给`create_dataset`函数来保存`points`张量：

```python
import h5py

f = h5py.File('../../data/chapter2/ourpoints.hdf5', 'w')
dset = f.create_dataset('coords', data=points.numpy())
f.close()
```

这里，`coords`是传入HDF5文件的键值。你还可以有其他键值，甚至是嵌套键值。HDF5中的一件有趣的事情是，你可以索引在磁盘的数据并且仅访问你感兴趣的元素。例如你只想加载数据集中的最后两个点数据：

```python
f = h5py.File('../../data/chapter2/ourpoints.hdf5', 'r')
dset = f['coords']
last_points = dset[1:]
```

上例中，当你打开文件或需要数据集时并未加载数据。相反，数据一直保留在磁盘上，直到你请求数据集中的第二行和最后一行。此时，`h5py`才访问这两行并返回了一个包含你想要数据的类似NumPy数组的对象，该对象的行为类似于NumPy数组，并且具有相同的API。

基于这个事实，你可以将返回的对象传递给`torch.from_numpy`函数以直接获取张量。需要注意的是，在这种情况下，数据将复制到张量存储中：

```python
last_points = torch.from_numpy(dset[1:])
f.close()
# last_points = torch.from_numpy(dset[1:]) # 会报错, 因为f已经关了
```

完成数据加载后，必须关闭文件。

## 9. 将张量转移到 GPU 上运行

**每一个 Torch 张量都可以转移到 GPU 上去执行快速、大规模并且可以并行的计算。在张量上执行的所有操作均由 PyTorch 自带的 GPU 特定例程执行。**

除了`dtype`之外，PyTorch张量还具有设备（`device`）的概念，这是在设置计算机上放张量（tensor）数据的位置。 通过为构造函数指定相应的参数，可以在GPU上创建张量：

```python
points_gpu = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]], device='cuda')
```

你可以使用`to`方法将在CPU上创建的张量（tensor）复制到GPU：

```python
points_gpu = points.to(device='cuda')
```

这段代码返回一个具有相同数值数据的新张量，但**存储在GPU的RAM中，而不是常规的系统RAM中**。

现在数据已经存放在本地的GPU中，当在张量上运行数字运算时，你可以看见很好的加速效果。并且，这个新 GPU 支持的张量的类也更改为`torch.cuda.FloatTensor`（一开始输入的类型为`torch.FloatTensor`；`torch.cuda.DoubleTensor`等等也存在对应关系）。在大部分样例中，基于CPU和GPU的张量都公开面向用户相同的API，这使得与繁琐数字运算过程中无关的代码的编写更加容易。

如果你的机器拥有多个 GPU，你可以通过传递从零开始的整数来确定张量分配给哪个 GPU，该整数标志着机器上的 GPU 下标： 

```python
points_gpu = points.to(device='cuda:0')
```

此时，在GPU上执行对张量的任何操作，例如将所有元素乘以一个常数。

```python
points = 2 * points # 在CPU上做乘法
points_gpu = 2 * points.to(device='cuda') # 在GPU上做乘法
```

请注意，当**计算结果产生后，`points_gpu`的张量并不会返回到CPU**。这里发生的是以下三个过程：

- 将`points`张量复制到 GPU

- 在GPU上分配了一个新的张量，并用于存储乘法的结果

- 返回该GPU张量的句柄

因此，如果你还想向结果加上一个常量：

```python
points_gpu = points_gpu + 4
```

加法仍然在GPU上执行，并且没有信息流到CPU（除非您打印或访问得到的张量）。 **如果要将张量移回CPU，你需要为`to`方法提供一个cpu参数**：

```python
points_cpu = points_gpu.to(device='cpu')
```

你可以使用速记方法`cpu`和`cuda`代替`to`方法来实现相同的目标

```python
points_gpu = points.cuda() # 默认为GPU0
points_gpu = points.cuda(0)
points_cpu = points_gpu.cpu()
```

值得一提的是，使用`to`方法时，可以通过提供`device`和`dtype`参数来同时更改位置和数据类型。

## 10. 就地运行

有少量的操作仅作为张量对象的方法存在。你可以通过名称中的下划线来识别它们，例如`zero_`，**下划线标识表明该方法是就地（inplace）运行的，即直接修改输入而不是创建新的输出并返回**。例如，`zero_`方法会将输入的所有元素清零。**任何不带下划线的方法都将保持源张量不变并返回新的张量**：

```python
a = torch.ones(3, 2)
a.zero_() # 就地运行
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201018203541.png" style="zoom:67%;" />

## 📚 References

- [DL-with-PyTorch-Chinese](https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/#/)

  <img src="https://gitee.com/veal98/images/raw/master/img/20201018102139.png" style="zoom:25%;" />

- [PyTorch 官方教程中文版](http://pytorch123.com/)

- [Pytorch 官方文档](https://pytorch.org/docs/stable/)