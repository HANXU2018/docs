# 🎉 自定义层和模型

---

[`tf.keras`](https://tensorflow.google.cn/api_docs/python/tf/keras?hl=zh_cn) 包含了各种内置层，例如：

- 卷积层：`Conv1D`、`Conv2D`、`Conv3D`、`Conv2DTranspose`
- 池化层：`MaxPooling1D`、`MaxPooling2D`、`MaxPooling3D`、`AveragePooling1D`
- RNN 层：`GRU`、`LSTM`、`ConvLSTM2D`
- `BatchNormalization`、`Dropout`、`Embedding` 等

但是，如果找不到所需内容，可以通过创建您自己的层来方便地扩展 API。所有层都会子类化 `Layer` 类并实现下列方法：

- `call` 方法，用于指定由层完成的计算。
- `build` 方法，用于创建层的权重（这只是一种样式约定，因为您也可以在 `__init__` 中创建权重）。

## 1. 创建层

Keras 的一个中心抽象是 `Layer` 类（状态（权重）和部分计算的组合）。层封装了状态（层的“权重”）和从输入到输出的转换（“调用”，即层的前向传递）。

下面是一个密集连接的层。它具有一个状态：变量 `w` 和 `b`。

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(input_dim, units), dtype="float32"),
            trainable=True, # 可训练权重
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(units,), dtype="float32"), trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```

您可以在某些张量输入上通过调用来使用层，这一点很像 Python 函数。

> 💡 层的 `__call__()` 方法将在首次调用时自动运行构建。

```python
x = tf.ones((2, 2))
linear_layer = Linear(4, 2)
y = linear_layer(x)
print(y)
tf.Tensor(
[[ 0.02562864 -0.09071901 -0.13720123  0.0189665 ]
 [ 0.02562864 -0.09071901 -0.13720123  0.0189665 ]], shape=(2, 4), dtype=float32)
```

请注意，**权重 `w` 和 `b` 在被设置为层特性后会由层自动跟踪**：

```python
assert linear_layer.weights == [linear_layer.w, linear_layer.b]
```

请注意，**您还可以使用一种更加快捷的方式为层添加权重：`add_weight()` 方法**：

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        self.w = self.add_weight(
            shape=(input_dim, units), initializer="random_normal", trainable=True
        )
        self.b = self.add_weight(shape=(units,), initializer="zeros", trainable=True)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b


x = tf.ones((2, 2))
linear_layer = Linear(4, 2)
y = linear_layer(x)
print(y)
tf.Tensor(
[[-0.05742075 -0.05801918 -0.06137164 -0.06648195]
 [-0.05742075 -0.05801918 -0.06137164 -0.06648195]], shape=(2, 4), dtype=float32)
```

## 2. 层可以具有不可训练权重

**除了可训练权重外，您还可以向层添加不可训练权重**。对层进行训练时，不必在反向传播期间考虑此类权重。

以下是添加和使用不可训练权重的方法 `trainable = False`：

```python
class ComputeSum(keras.layers.Layer):
    def __init__(self, input_dim):
        super(ComputeSum, self).__init__()
        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)

    def call(self, inputs):
        self.total.assign_add(tf.reduce_sum(inputs, axis=0))
        return self.total


x = tf.ones((2, 2))
my_sum = ComputeSum(2)
y = my_sum(x)
print(y.numpy())
y = my_sum(x)
print(y.numpy())
[2. 2.]
[4. 4.]
```

它是 `layer.weights` 的一部分，但被归类为不可训练权重：

```python
print("weights:", len(my_sum.weights))
print("non-trainable weights:", len(my_sum.non_trainable_weights))

# It's not included in the trainable weights:
print("trainable_weights:", my_sum.trainable_weights)
weights: 1
non-trainable weights: 1
trainable_weights: []
```

## 3. 最佳做法：将权重创建推迟到得知输入的形状之后

上面的 `Linear` 层接受了一个 `input_dim` 参数，用于计算 `__init__()` 中权重 `w` 和 `b` 的形状：

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        self.w = self.add_weight(
            shape=(input_dim, units), initializer="random_normal", trainable=True
        )
        self.b = self.add_weight(shape=(units,), initializer="zeros", trainable=True)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```

<u>在许多情况下，您可能事先不知道输入的大小，并希望在得知该值时（对层进行实例化后的某个时间）再延迟创建权重。</u>

🚩 **在 Keras API 中，我们建议您在层的 `build(self, inputs_shape)` 方法中创建层的权重**。如下所示：

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32):
        super(Linear, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer="random_normal",
            trainable=True,
        )
        self.b = self.add_weight(
            shape=(self.units,), initializer="random_normal", trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```

现在，您有了一个延迟并因此更易使用的层：

```python
# At instantiation, we don't know on what inputs this is going to get called
linear_layer = Linear(32)

# The layer's weights are created dynamically the first time the layer is called
y = linear_layer(x)
```

## 4. 层可递归组合

如果将一个层实例分配为另一个层的特性，则外部层将开始跟踪内部层的权重。

我们建议在 `__init__()` 方法中创建此类子层（由于子层通常具有构建方法，它们将与外部层同时构建）。

```python
# Let's assume we are reusing the Linear class
# with a `build` method that we defined above.


class MLPBlock(keras.layers.Layer):
    def __init__(self):
        super(MLPBlock, self).__init__()
        self.linear_1 = Linear(32)
        self.linear_2 = Linear(32)
        self.linear_3 = Linear(1)

    def call(self, inputs):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.linear_2(x)
        x = tf.nn.relu(x)
        return self.linear_3(x)


mlp = MLPBlock()
y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights
print("weights:", len(mlp.weights))
print("trainable weights:", len(mlp.trainable_weights))
weights: 6
trainable weights: 6
```

## 5. `add_loss()` 方法

在编写层的 `call()` 方法时，您可以在编写训练循环时创建需要稍后使用的损失张量。这可以通过调用 `self.add_loss(value)` 来实现：

```python
# A layer that creates an activity regularization loss
class ActivityRegularizationLayer(keras.layers.Layer):
    def __init__(self, rate=1e-2):
        super(ActivityRegularizationLayer, self).__init__()
        self.rate = rate

    def call(self, inputs):
        self.add_loss(self.rate * tf.reduce_sum(inputs))
        return inputs
```

这些损失（包括由任何内部层创建的损失）可通过 `layer.losses` 取回。此属性会在每个 `__call__()` 开始时重置到顶层，因此 **`layer.losses` 始终包含在上一次前向传递过程中创建的损失值**。

```python
class OuterLayer(keras.layers.Layer):
    def __init__(self):
        super(OuterLayer, self).__init__()
        self.activity_reg = ActivityRegularizationLayer(1e-2)

    def call(self, inputs):
        return self.activity_reg(inputs)


layer = OuterLayer()
assert len(layer.losses) == 0  # No losses yet since the layer has never been called

_ = layer(tf.zeros(1, 1))
assert len(layer.losses) == 1  # We created one loss value

# `layer.losses` gets reset at the start of each __call__
_ = layer(tf.zeros(1, 1))
assert len(layer.losses) == 1  # This is the loss created during the call above
```

此外，`loss` 属性还包含为任何内部层的权重创建的正则化损失：

```python
class OuterLayerWithKernelRegularizer(keras.layers.Layer):
    def __init__(self):
        super(OuterLayerWithKernelRegularizer, self).__init__()
        self.dense = keras.layers.Dense(
            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)
        )

    def call(self, inputs):
        return self.dense(inputs)


layer = OuterLayerWithKernelRegularizer()
_ = layer(tf.zeros((1, 1)))

# This is `1e-3 * sum(layer.dense.kernel ** 2)`,
# created by the `kernel_regularizer` above.
print(layer.losses)
[<tf.Tensor: shape=(), dtype=float32, numpy=0.0013254517>]
```

这些损失还可以无缝使用 `fit()`（它们会自动求和并添加到主损失中，如果有的话）：

```python
import numpy as np

inputs = keras.Input(shape=(3,))
outputs = ActivityRegularizationLayer()(inputs)
model = keras.Model(inputs, outputs)

# If there is a loss passed in `compile`, thee regularization
# losses get added to it
model.compile(optimizer="adam", loss="mse")
model.fit(np.random.random((2, 3)), np.random.random((2, 3)))

# It's also possible not to pass any loss in `compile`,
# since the model already has a loss to minimize, via the `add_loss`
# call during the forward pass!
model.compile(optimizer="adam")
model.fit(np.random.random((2, 3)), np.random.random((2, 3)))
1/1 [==============================] - 0s 1ms/step - loss: 0.2453
1/1 [==============================] - 0s 1ms/step - loss: 0.0325

<tensorflow.python.keras.callbacks.History at 0x7fd6fc212240>
```

## 6. `add_metric()` 方法

与 `add_loss()` 类似，层还具有 `add_metric()` 方法，用于在训练过程中跟踪数量的移动平均值。

请思考下面的 "logistic endpoint" 层。它将预测和目标作为输入，计算通过 `add_loss()` 跟踪的损失，并计算通过 `add_metric()` 跟踪的准确率标量。

```python
class LogisticEndpoint(keras.layers.Layer):
    def __init__(self, name=None):
        super(LogisticEndpoint, self).__init__(name=name)
        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)
        self.accuracy_fn = keras.metrics.BinaryAccuracy()

    def call(self, targets, logits, sample_weights=None):
        # Compute the training-time loss value and add it
        # to the layer using `self.add_loss()`.
        loss = self.loss_fn(targets, logits, sample_weights)
        self.add_loss(loss)

        # Log accuracy as a metric and add it
        # to the layer using `self.add_metric()`.
        acc = self.accuracy_fn(targets, logits, sample_weights)
        self.add_metric(acc, name="accuracy")

        # Return the inference-time prediction tensor (for `.predict()`).
        return tf.nn.softmax(logits)
```

可通过 `layer.metrics` 访问以这种方式跟踪的指标：

```python
layer = LogisticEndpoint()

targets = tf.ones((2, 2))
logits = tf.ones((2, 2))
y = layer(targets, logits)

print("layer.metrics:", layer.metrics)
print("current accuracy value:", float(layer.metrics[0].result()))
layer.metrics: [<tensorflow.python.keras.metrics.BinaryAccuracy object at 0x7fd709b24240>]
current accuracy value: 1.0
```

和 `add_loss()` 一样，这些指标也是通过 `fit()` 跟踪的：

```python
inputs = keras.Input(shape=(3,), name="inputs")
targets = keras.Input(shape=(10,), name="targets")
logits = keras.layers.Dense(10)(inputs)
predictions = LogisticEndpoint(name="predictions")(logits, targets)

model = keras.Model(inputs=[inputs, targets], outputs=predictions)
model.compile(optimizer="adam")

data = {
    "inputs": np.random.random((3, 3)),
    "targets": np.random.random((3, 10)),
}
model.fit(data)
1/1 [==============================] - 0s 2ms/step - loss: 1.0657 - binary_accuracy: 0.0000e+00

<tensorflow.python.keras.callbacks.History at 0x7fd6fc0cca20>
```

## 7. `Model` 类

**通常，您会使用 `Layer` 类来定义内部计算块，并使用 `Model` 类来定义外部模型，即您将训练的对象**。

例如，在 ResNet50 模型中，您会有几个子类化 `Layer` 的 ResNet 块，和一个包含了整个 ResNet50 网络的单个 `Model`。

`Model` 类具有与 `Layer` 相同的 API，但有如下区别：

- 它会公开内置训练、评估和预测循环（`model.fit()`、`model.evaluate()`、`model.predict()`）。
- 它会通过 `model.layers` 属性公开其内部层的列表。
- 它会公开保存和序列化 API（`save()`、`save_weights()`…）

实际上，**`Layer` 类对应于我们在文献中所称的 “层” **（如“卷积层”或“循环层”）或“块”（如“ResNet 块”或“Inception 块”）。

同时，**`Model` 类对应于文献中所称的 “模型” **（如“深度学习模型”）或“网络”（如“深度神经网络”）。

因此，如果您想知道“我应该用 `Layer` 类还是 `Model` 类？”，请问自己：我是否需要在它上面调用 `fit()`？我是否需要在它上面调用 `save()`？如果是，则使用 `Model`。如果不是（要么因为您的类只是更大系统中的一个块，要么因为您正在自己编写训练和保存代码），则使用 `Layer`。

例如，我们可以拿上面的 mini-resnet 示例为例，用它来构建一个 `Model`，该模型可以通过 `fit()` 进行训练，并可以通过 `save_weights()` 来保存：

```python
class ResNet(tf.keras.Model):      
    def __init__(self):         
        super(ResNet, self).__init__()         
        self.block_1 = ResNetBlock()         
        self.block_2 = ResNetBlock()         
        self.global_pool = layers.GlobalAveragePooling2D()         
        self.classifier = Dense(num_classes)   
        
    def call(self, inputs):         
        x = self.block_1(inputs)         
        x = self.block_2(x)         
        x = self.global_pool(x)         
        return self.classifier(x)   
    
resnet = ResNet() 
dataset = resnet.fit(dataset, epochs=10) 
resnet.save(filepath)
```

## 8. 小结

到目前为止，您已学习以下内容：

- `Layer` 封装了（在 `__init__()` 或 `build()` 中创建的）状态和（在 `call()` 中定义的）部分计算。
- 层可以递归嵌套以创建新的更大的计算块。
- 层可以通过 `add_loss()` 和 `add_metric()` 创建并跟踪损失（通常是正则化损失）以及指标。
- 您要训练的外部容器是 `Model`。`Model` 就像 `Layer`，但是添加了训练和序列化实用工具。

## 📚 References

- [TensorFlow 2 官方文档](https://tensorflow.google.cn/tutorials/keras/classification?hl=zh_cn)
- [TensorFlow 2 官方指南](https://tensorflow.google.cn/guide/tensor?hl=zh_cn#%E6%93%8D%E4%BD%9C%E5%BD%A2%E7%8A%B6)