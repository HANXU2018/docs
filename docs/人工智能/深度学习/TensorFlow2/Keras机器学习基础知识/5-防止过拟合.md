# 🍚 防止过拟合

---

```python
import tensorflow
from tensorflow import keras
```

## 1. Add Weight Regularization

相比较来说，较简单的模型比复杂的模型更不容易发生过拟合。

将模型变简单，除了将模型变小（减少网络层数和每层神经元个数）以外，还有另一种方式，减小模型权重(w)的熵(entropy)。即限制权重值在一个较小的范围内，这样模型中权重分布看起来更“regular”，这被称为“`权重/参数正则化(weight regularization)`”，它是通过向网络的损失函数 loss 中添加与权重相关的代价 cost 来完成的。 有两种代价 cost，也即两种正则化方式：

- [L1 regularization](https://developers.google.cn/machine-learning/glossary/?hl=zh_cn#L1_regularization), where the cost added is proportional（正比） to the <u>absolute value (绝对值)</u> of the <u>weights coefficients (权重系数)</u>
- [L2 regularization](https://developers.google.cn/machine-learning/glossary/?hl=zh_cn#L2_regularization), where the cost added is proportional to <u>the square of the value of the weights coefficients</u> .  L2 正则化在神经网络中也被称为 “权重衰减”

L1 正则化会将权重推向为 0，从而鼓励稀疏模型。而 L2 正则化虽然惩罚参数，但是对于小权重来说，不会使他们变为 0，所以 **L2 正则化更为常见**。

In `tf.keras`, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization now 👇

```python
l2_model = tf.keras.Sequential([
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001),
                 input_shape=(FEATURES,)),
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(1)
])
```

`l2(0.001)` means that <u>every coefficient in the weight matrix of the layer (该层的权重矩阵中的每个系数)</u> will add `0.001 * weight_coefficient_value**2` to the total **loss** of the network.

因此，具有 L2 正则化惩罚的相同“大型”模型的性能要好得多

## 2. Add Dropout

在神经网络中，Dropout是最有效的以及使用最广泛的正则化方式。Dropout作用在网络层，训练过程中随机丢弃(dropping out)一部分输出值（例如置为0），Dropout的比例一般置为0.2到0.5之间。例如：

```
[0.2, 0.3, 0.5, 0.7, 0.9]

# after 40% dropout

[0.2, 0, 0.5, 0.7, 0]
```

<u>In [`tf.keras`](https://tensorflow.google.cn/api_docs/python/tf/keras?hl=zh_cn) you can introduce dropout in a network via the Dropout layer, which gets applied to the output of layer right before.</u>

```python
dropout_model = tf.keras.Sequential([
    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),
    layers.Dropout(0.5),
    layers.Dense(512, activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(1)
])
```

## 3. Combined L2 + Dropout

```python
combined_model = tf.keras.Sequential([
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu', input_shape=(FEATURES,)),
    layers.Dropout(0.5),
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(1)
])
```



## 📚 References

- [TensorFlow 2 官方文档](https://tensorflow.google.cn/tutorials/keras/classification?hl=zh_cn)
- [TensorFlow 2 官方指南](https://tensorflow.google.cn/guide/tensor?hl=zh_cn#%E6%93%8D%E4%BD%9C%E5%BD%A2%E7%8A%B6)