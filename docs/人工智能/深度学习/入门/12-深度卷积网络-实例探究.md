# 🌅 深度卷积网络：实例探究

---

## 1. 为什么要进行实例探究

本周课程将主要介绍几个典型的 CNN 案例。**通过对具体 CNN 模型及案例的研究，来帮助我们理解知识并训练实际的模型**。

典型的 CNN 模型包括：

- LeNet-5
- AlexNet
- VGG

除了这些性能良好的CNN模型之外，我们还会介绍 残差网络 Residual Network（ResNet）。其特点是可以构建很深很深的神经网络（目前最深的好像有152层）。另外，还会介绍 Inception Neural Network。接下来，我们将一一讲解。

## 2. 经典网络 Classic networks

这节课，我们来学习几个经典的神经网络结构，分别是 **LeNet-5**、**AlexNet** 和 **VGG**：

### ① LeNet-5

LeNet-5 模型是 Yann LeCun 教授于1998年提出来的，它是第一个成功应用于数字识别问题的卷积神经网络。在MNIST数据中，它的准确率达到大约99.2%。

典型的 LeNet-5 结构包含 CONV layer，POOL layer 和 FC layer，顺序一般是 **CONV layer -> POOL layer -> CONV layer -> POOL layer -> FC layer -> FC layer -> OUTPUT layer (即 $\hat y$)**，

下图所示的是一个数字识别的 LeNet-5 的模型结构：

![](https://gitee.com/veal98/images/raw/master/img/20201011094810.png)

该LeNet模型总共包含了大约6万个参数。值得一提的是，当时 **Yann LeCun 提出的 LeNet-5 模型池化层使用的是 average pooling，而且各层激活函数一般是 Sigmoid 和 tanh**。现在，我们可以根据需要，做出改进，使用 max pooling 和激活函数 ReLU 👇

### ② AlexNet

AlexNet模型是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示：

![](https://gitee.com/veal98/images/raw/master/img/20201011095432.png)

AlexNet模型与LeNet-5模型类似，只是要复杂一些，总共包含了大约6千万个参数。当用于训练图像和数据集时，**AlexNet**能够处理非常相似的基本构造模块，这些模块往往包含着大量的隐藏单元或数据，这一点**AlexNet**表现出色。**AlexNet 比 LeNet 表现更为出色的另一个原因是它使用了 ReLu 激活函数**。

### ③ VGG

**VGG（也称 VGG-16）** 模型更加复杂一些，一般情况下，其 CONV layer 和 POOL layer 设置如下：

- CONV = 3×3 filters, s = 1, same
- MAX-POOL = 2 × 2, s = 2

VGG-16 结构如下所示：

![](https://gitee.com/veal98/images/raw/master/img/20201011095849.png)

**VGG-16**网络没有那么多超参数，这是一种只需要专注于构建卷积层的简单网络。首先用3×3，步幅为1的过滤器构建卷积层。然后用一个2×2，步幅为2的过滤器构建最大池化层。因此**VGG**网络的一大优点是它确实简化了神经网络结构，下面我们具体讲讲这种网络结构。

顺便说一下，<u>**VGG-16** 的这个数字16，就是指在这个网络中包含16个卷积层和全连接层</u>。确实是个很大的网络，总共包含约1.38亿个参数。但**VGG-16**的结构并不复杂，这点非常吸引人，<u>而且这种网络结构很规整，都是几个卷积层后面跟着可以压缩大小的池化层，池化层缩小图像的高度和宽度</u>。同时，卷积层的过滤器数量变化存在一定的规律，由64翻倍变成128，再到256和512。作者可能认为512已经足够大了，所以后面的层就不再翻倍了。这种相对一致的网络结构对研究者很有吸引力，而它的主要缺点是需要训练的特征数量非常巨大。

> 💡 还有**VGG-19**网络，它甚至比**VGG-16**还要大

## 3. 残差网络 Residual Networks (ResNets)

我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是**人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系**。这种神经网络被称为 **残差网络 `Residual Networks(ResNets)`**。

**Residual Networks 由许多隔层相连的神经元子模块组成，我们称之为 `Residual block`**。单个 Residual block的结构如下图所示：

![](https://gitee.com/veal98/images/raw/master/img/20201011101033.png)

上图中红色部分就是 `skip connection`，直接建立 $a^{[l]}$ 与 $a^{[l+2]}$ 之间的隔层联系。相应的表达式如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20201011101151.png" style="zoom:67%;" />

$a^{[l]}$ 直接隔层与下一层的线性输出相连，与 $z^{[l+2]}$ 共同通过激活函数（ReLU）输出 $a^{[l+2]}$ 。

该模型由 何凯明（Kaiming He）、张翔宇（Xiangyu Zhang）、任少卿（Shaoqing Ren）和孙剑（Jiangxi Sun）共同提出。由多个 Residual block 组成的神经网络就是 Residual Network。实验表明，这种模型结构对于训练非常深的神经网络，效果很好。Residual Network 的结构如下图所示：

![](https://gitee.com/veal98/images/raw/master/img/20201011101459.png)

另外，为了便于区分，我们把非 Residual Networks 称为 **普通网络（Plain network）**。

与 Plain Network 相比，**Residual Network 能够训练更深层的神经网络，有效避免发生发生梯度消失和梯度爆炸**。从下面两张图的对比中可以看出，随着神经网络层数增加，Plain Network 实际性能会变差，training error 甚至会变大。然而，Residual Network 的训练效果却很好，training error 一直呈下降趋势：

<img src="https://gitee.com/veal98/images/raw/master/img/20201011101627.png" style="zoom:80%;" />

## 4. 残差网络为什么有用

下面用个例子来解释为什么 ResNets 能够训练更深层的神经网络：

<img src="https://gitee.com/veal98/images/raw/master/img/20201011102013.png" style="zoom:80%;" />

如上图所示，输入 x 经过很多层神经网络后输出 $a^{[l]}$ ，$a^{[l]}$ 经过一个 Residual block 输出 $a^{[l+2]}$ 。$a^{[l+2]}$ 的表达式为：

$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$ 

输入 x 经过 Big NN 后，若 $W^{[l+2]}\approx0$ ，$b^{[l+2]}\approx0$ ，则有：

$a^{[l+2]}=g(a^{[l]})=ReLU(a^{[l]})=a^{[l]}\ \ \ \ when\ a^{[l]}\geq0$ 

👍 可以看出，即使发生了梯度消失，$W^{[l+2]}\approx0$ ，$b^{[l+2]}\approx0$ ，也能直接建立 $a^{[l+2]}$ 与 $a^{[l]}$ 的线性关系，且$a^{[l+2]}=a^{[l]}$ ，这其实就是 **恒等函数 identity function**。$a^{[l]}$ 直接连到 $a^{[l+2]}$ ，从效果来说，相当于直接忽略了$a^{[l]}$ 之后的这两层神经层。这样，看似很深的神经网络，其实由于许多 Residual blocks的存在，弱化削减了某些神经层之间的联系，实现隔层线性传递，而不是一味追求非线性关系，模型本身也就能“容忍”更深层的神经网络了。而且从性能上来说，这两层额外的 Residual blocks 也不会降低 Big NN 的性能。

当然，如果 Residual blocks 确实能训练得到非线性关系，那么也会忽略 skip connection，跟 Plain Network 起到同样的效果。

有一点需要注意的是，**如果 Residual blocks 中 $a^{[l]}$ 和 $a^{[l+2]}$ 的维度不同，通常可以引入矩阵 $W_s$ ，与 $a^{[l]}$ 相乘，使得 $W_s*a^{[l]}$ 的维度与 $a^{[l+2]}$ 一致**。参数矩阵 $W_s$ 有来两种方法得到：一种是将 $W_s$ 作为学习参数，通过模型训练得到；另一种是固定 $W_s$ 值（类似单位矩阵），不需要训练，$W_s$ 与 $a^{[l]}$ 的乘积仅仅使得 $a^{[l]}$ 截断或者补零。这两种方法都可行。

下图所示的是 CNN 中 ResNets 的结构：

![](https://gitee.com/veal98/images/raw/master/img/20201011102527.png)

**ResNets 同类型层之间，例如 CONV layers，大多使用 same 类型，保持维度相同。如果是不同类型层之间的连接，例如 CONV layer 与 POOL layer 之间，如果维度不同，则引入矩阵 $W_s$ 。**

## 5. 网络中的网络 / 1×1 卷积

Min Lin, Qiang Chen等人提出了一种新的 CNN结构，即**1×1 Convolutions**，也称 **Networks in Networks**。这种结构的特点是**滤波器算子 filter 的维度为1×1**。对于单个 filter，1×1 的维度，**意味着卷积操作等同于乘积操作**。

![](https://gitee.com/veal98/images/raw/master/img/20201011103007.png)

那么，对于多个filters，1×1 Convolutions的作用实际上类似全连接层的神经网络结构。效果等同于 Plain Network中 $a^{[l]}$ 到 $a^{[l+1]}$ 的过程。

![](https://gitee.com/veal98/images/raw/master/img/20201011103032.png)

**1×1 Convolutions可以用来缩减输入图片的通道数目**。方法如下图所示（32 表示 过滤器数量）：

![](https://gitee.com/veal98/images/raw/master/img/20201011103046.png)

## 6. 谷歌 Inception 网络简介

之前我们介绍的CNN单层的滤波算子filter尺寸是固定的，1×1或者3×3等。而 **Inception Network 在单层网络上可以使用多个不同尺寸的 filters，进行 same convolutions，把各 filter 下得到的输出拼接起来**。除此之外，还可以将CONV layer与POOL layer混合，同时实现各种效果。但是要注意使用same pool。

<img src="https://gitee.com/veal98/images/raw/master/img/20201011104020.png" style="zoom:80%;" />

Inception Network 由 Christian Szegedy, Wei Liu 等人提出。与其它只选择单一尺寸和功能的 filter 不同，Inception Network 使用不同尺寸的 filters 并将 CONV 和 POOL 混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块。

Inception Network 在提升性能的同时，会带来计算量大的问题。例如下面这个例子：

![](https://gitee.com/veal98/images/raw/master/img/20201011104053.png)

此CONV layer需要的计算量为：28x28x32x5x5x192=120m，其中m表示百万单位。可以看出但这一层的计算量都是很大的。**为此，我们可以引入1×1 Convolutions来减少其计算量**，结构如下图所示：

![](https://gitee.com/veal98/images/raw/master/img/20201011104159.png)

通常我们把该 1×1 Convolution 称为 **瓶颈层（bottleneck layer）**。引入 bottleneck layer 之后，总共需要的计算量为：28x28x16x192 + 28x28x32x5x5x16=12.4m。明显地，虽然多引入了1×1 Convolution层，但是总共的计算量减少了近 90%，效果还是非常明显的。**由此可见，1×1 Convolutions还可以有效减少 CONV layer 的计算量**。

## 7. Inception 网络

上一节我们使用1×1 Convolution 来减少Inception Network计算量大的问题。引入1×1 Convolution 后的 `Inception module` 如下图所示：

![](https://gitee.com/veal98/images/raw/master/img/20201011104440.png)

**多个 Inception modules 组成 `Inception Network`**，效果如下图所示：

![](https://gitee.com/veal98/images/raw/master/img/20201011104509.png)

上述 Inception Network 除了由许多 Inception modules 组成之外，**值得一提的是该网络中的隐藏层也可以作为输出层预测分类，有利于防止发生过拟合**。

## 8. 数据扩充 Data augmentation

大部分的计算机视觉任务使用很多的数据，所以数据扩充是经常使用的一种技巧来提高计算机视觉系统的表现。

常用的Data Augmentation方法包括对已有样本集合的 **镜像 Mirroring** 和 **随机裁剪 Random Cropping** 以及 **彩色转换 Color Shifting**：

- 🔸 **Mirroring**

  ![](https://gitee.com/veal98/images/raw/master/img/20201011104841.png)

- 🔸 **Random Cropping**

  <img src="https://gitee.com/veal98/images/raw/master/img/20201011104846.png" style="zoom:80%;" />

- 🔸 **Color Shifting**：color shifting 就是对图片的RGB通道数值进行随意增加或者减少，改变图片色调。

  ![](https://gitee.com/veal98/images/raw/master/img/20201011105027.png)



另一种Data Augmentation的方法是color shifting。color shifting就是对图片的RGB通道数值进行随意增加或者减少，改变图片色调。

## 📚 Reference

- [黄海广 - Coursera 深度学习教程中文笔记](https://github.com/fengdu78/deeplearning_ai_books)
- [红色石头 - 吴恩达 deeplearning.ai 专项课程精炼笔记](https://redstonewill.com/category/ai-notes/andrew-deeplearning-ai/page/2/)